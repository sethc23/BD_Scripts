{
 "metadata": {
  "name": "",
  "signature": "sha256:8dff054cf9c3784c3a3dc4b9317efb5d1e466d73c18256d9d1a231dddd6c8385"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "# add encoding?\n",
      "import sys\n",
      "import codecs\n",
      "reload(sys)\n",
      "sys.setdefaultencoding('UTF8') # to set\n",
      "# sys.setdefaultencoding('ascii') # to unset"
     ],
     "language": "python",
     "metadata": {
      "code_folding": [
       0
      ],
      "run_control": {
       "read_only": false
      }
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ibk"
     ],
     "language": "python",
     "metadata": {
      "code_folding": [],
      "run_control": {
       "read_only": false
      }
     },
     "outputs": [],
     "prompt_number": 50
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Libraries\n",
      "THIS_NOTEBOOK = 'aporo_vendor_info'\n",
      "import pandas as pd\n",
      "pd.set_option('expand_frame_repr',False)\n",
      "pd.set_option('display.max_columns', None)\n",
      "pd.set_option('display.max_rows', 1000)\n",
      "pd.set_option('display.width',180)\n",
      "np = pd.np\n",
      "np.set_printoptions(linewidth=200,threshold=np.nan)\n",
      "from datetime import datetime as dt\n",
      "from types import NoneType\n",
      "from time import sleep as delay\n",
      "import codecs\n",
      "from sqlalchemy import create_engine\n",
      "#from logging import getLogger\n",
      "#from logging import INFO as logging_info\n",
      "#getLogger('sqlalchemy.dialects.postgresql').setLevel(logging_info)\n",
      "engine = create_engine(r'postgresql://postgres:postgres@192.168.3.52:8800/routing',\n",
      "                       encoding='utf-8',\n",
      "                       echo=False)\n",
      "\n",
      "%load_ext sql\n",
      "# %sql postgresql://postgres:postgres@localhost/routing\n",
      "%sql postgresql://postgres:postgres@192.168.3.52:8800/routing\n",
      "import psycopg2\n",
      "conn = psycopg2.connect(\"dbname='routing' user='postgres' host='192.168.3.52' password='' port=8800\");\n",
      "cur = conn.cursor()\n",
      "from time import sleep as delay\n",
      "from os import path as os_path\n",
      "from os import system as os_cmd\n",
      "from pandas.tseries.offsets import DateOffset\n",
      "mn_zipcodes = [10001, 10002, 10003, 10004, 10005, 10006, \n",
      "               10007, 10009, 10010, 10011, 10012, 10013, \n",
      "               10014, 10015, 10016, 10017, 10018, 10019, \n",
      "               10020, 10021, 10022, 10023, 10024, 10025, \n",
      "               10026, 10027, 10028, 10029, 10030, 10031, \n",
      "               10032, 10033, 10034, 10035, 10036, 10037, \n",
      "               10038, 10039, 10040, 10041, 10043, 10044, \n",
      "               10045, 10046, 10047, 10048, 10055, 10060, \n",
      "               10065, 10069, 10072, 10075, 10079, 10080, \n",
      "               10081, 10082, 10087, 10090, 10094, 10095, \n",
      "               10096, 10098, 10099, 10102, 10103, 10104, \n",
      "               10105, 10106, 10107, 10109, 10110, 10111, \n",
      "               10112, 10114, 10115, 10117, 10118, 10119, \n",
      "               10120, 10121, 10122, 10123, 10124, 10125, \n",
      "               10126, 10128, 10130, 10131, 10132, 10133, \n",
      "               10138, 10149, 10151, 10152, 10153, 10154, \n",
      "               10155, 10157, 10158, 10160, 10161, 10162, \n",
      "               10164, 10165, 10166, 10167, 10168, 10169, \n",
      "               10170, 10171, 10172, 10173, 10174, 10175, \n",
      "               10176, 10177, 10178, 10179, 10184, 10196, \n",
      "               10197, 10199, 10203, 10211, 10212, 10213, \n",
      "               10256, 10257, 10258, 10259, 10260, 10261, \n",
      "               10265, 10269, 10270, 10271, 10273, 10275, \n",
      "               10277, 10278, 10279, 10280, 10281, 10282, \n",
      "               10285, 10286, 10292]"
     ],
     "language": "python",
     "metadata": {
      "code_folding": [
       0
      ],
      "run_control": {
       "read_only": false
      }
     },
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Database Info/Analysis"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "# seamless analysis...\n",
      "\n",
      "# CLEAN UP FUNCTIONS:\n",
      "    \n",
      "    # 1. make sure 'inactive' is not null\n",
      "conn.set_isolation_level(0)\n",
      "cur.execute(\"\"\"update seamless set inactive = false where inactive is null\"\"\")\n",
      "    \n",
      "    # 2. mark any entries without links updated as 'updated' if upd_vend_content is not null\n",
      "conn.set_isolation_level(0)\n",
      "cur.execute(\"\"\"\n",
      "    update seamless\n",
      "    set upd_search_links = upd_vend_content\n",
      "    where \n",
      "    (upd_search_links is null and upd_vend_content is not null)\n",
      "    or\n",
      "    (upd_search_links < upd_vend_content);\n",
      "\"\"\")   \n",
      "\n",
      "    # 3. update seamless inactive from seamless_closed\n",
      "conn.set_isolation_level(0)\n",
      "cur.execute(\"\"\"\n",
      "    update seamless s\n",
      "    set inactive = true\n",
      "    from seamless_closed sc\n",
      "    where s.sl_link = sc.sl_link\n",
      "    and sc.inactive is true;\n",
      "\"\"\")\n",
      "    \n",
      "    # 4. delete from seamless_closed where updated seamless\n",
      "conn.set_isolation_level(0)\n",
      "cur.execute(\"\"\"\n",
      "update seamless_closed sc\n",
      "set vend_name = null\n",
      "from seamless s\n",
      "where s.sl_link = sc.sl_link\n",
      "and sc.upd_seamless is true;\n",
      "\n",
      "delete from seamless_closed sc\n",
      "where vend_name is null;\n",
      "\"\"\")\n",
      "\n",
      "    # 5. remove entries outside of Manhattan\n",
      "\n",
      "# conn.set_isolation_level(0)\n",
      "# cur.execute(delete_cmd)\n",
      "\n",
      "cmd = \"\"\"\n",
      "select s.* from seamless s, (select array_agg(B::integer) A from unnest(array%(mn_zipcodes)s) B) as f1 \n",
      "where not array[s.zipcode] && A order by s.zipcode;\"\"\"%{'mn_zipcodes':mn_zipcodes}\n",
      "test = pd.read_sql(cmd,engine)\n",
      "if len(test)>0:\n",
      "    print \"\"\"Really delete out-of-area rows?\\n\n",
      "             Use This:\\n\\n\n",
      "             conn.set_isolation_level(0)\n",
      "             cur.execute(delete_cmd)\\n\"\"\"\n",
      "    delete_cmd = \"\"\"\n",
      "        delete from seamless s\n",
      "        using (select array_agg(B::integer) A from unnest(array%(mn_zipcodes)s) B) as f1 \n",
      "        where not array[s.zipcode] && A;\"\"\"%{'mn_zipcodes':mn_zipcodes}\n",
      "    raise SystemError()\n",
      "    \n",
      "    \n",
      "print 'DB -- seamless'\n",
      "d = pd.DataFrame(columns=['Value','Var'])\n",
      "\n",
      "sl = pd.read_sql('select * from seamless',engine)\n",
      "sl_all_vend = len(sl)\n",
      "\n",
      "sl_link_updates = sl[sl.upd_search_links.map(lambda s: True if s!=None else False)==True].sort('upd_search_links',ascending=True).upd_search_links.tolist()\n",
      "oldest_sl_link_update,newest_sl_link_update = sl_link_updates[0],sl_link_updates[-1]\n",
      "sl_null_link_updates = sl_all_vend - len(sl_link_updates)\n",
      "\n",
      "sl_content_updates = sl[sl.upd_vend_content.map(lambda s: True if s!=None else False)==True].sort('upd_vend_content',ascending=True).upd_vend_content.tolist()\n",
      "oldest_sl_content_update,newest_sl_content_update = sl_content_updates[0],sl_content_updates[-1]\n",
      "sl_null_content_updates = sl_all_vend - len(sl_content_updates)\n",
      "\n",
      "sl_not_updated = (sl_all_vend-(sl_null_link_updates+sl_null_content_updates))\n",
      "sl_perc_updated = str(round(100 * (float(sl_not_updated)/float(sl_all_vend)),2))+' %'\n",
      "\n",
      "\n",
      "sl_updated = sl[(sl.upd_search_links.map(lambda s: True if s!=None else False)==True) & \n",
      "                (sl.upd_vend_content.map(lambda s: True if s!=None else False)==True)].copy().reset_index(drop=True)\n",
      "\n",
      "sl_all_upd_vend = len(sl_updated)\n",
      "sl_uniq_vend = len(sl_updated.vend_id.unique().tolist())\n",
      "\n",
      "sl_active_vend = len(sl_updated[sl_updated.inactive==False])\n",
      "sl_inactive_vend = len(sl_updated[sl_updated.inactive==True])\n",
      "sl_perc_active_vend= str(round(100 * (float(sl_active_vend)/float(sl_all_upd_vend)),2))+' %'\n",
      "\n",
      "sl_with_geom = len(sl_updated[sl_updated.geom.isnull()==False])\n",
      "sl_WITHOUT_geom = sl_all_upd_vend - sl_with_geom\n",
      "sl_perc_with_geom = str(round(100 * (float(sl_with_geom)/float(sl_all_upd_vend)),2))+' %'\n",
      "\n",
      "sl_with_matched_bbl = sl_updated.bbl.map(lambda s: True if str(s).lower()!='nan' else False).tolist().count(True)\n",
      "sl_perc_with_matched_bbl = str(round(100 * (float(sl_with_matched_bbl)/float(sl_all_upd_vend)),2))+' %'\n",
      "sl_WITHOUT_matched_bbl = sl_all_upd_vend - sl_with_matched_bbl\n",
      "\n",
      "\n",
      "params = [\n",
      "    ('Oldest SL Link Update',oldest_sl_link_update),\n",
      "    ('Newest SL Link Update',newest_sl_link_update),\n",
      "    ('Oldest SL Vend Content Update',oldest_sl_content_update),\n",
      "    ('Newest SL Vend Content Update',newest_sl_content_update),\n",
      "    ('# of Seamless Vendors',sl_all_vend),\n",
      "    ('# of Duplicate VEND Rows',len(sl_updated) - sl_uniq_vend),\n",
      "    ('% of Updated Seamless Vendors (\"VEND\")',sl_perc_updated),\n",
      "    ('# of VEND without Links Updated',sl_null_link_updates),\n",
      "    ('# of VEND without Content Updated',sl_null_content_updates),\n",
      "    ('% of Active Seamless VEND',sl_perc_active_vend),\n",
      "    ('# of Inactive Seamless VEND',sl_inactive_vend),\n",
      "    ('% of VEND with geom',sl_perc_with_geom),\n",
      "    ('# WITHOUT geom',sl_WITHOUT_geom),\n",
      "    ('% of VEND with BBL',sl_perc_with_matched_bbl),\n",
      "    ('# WITHOUT BBL',sl_WITHOUT_matched_bbl),\n",
      "         ]\n",
      "\n",
      "for it in params:\n",
      "    d = d.append({'Var':it[0],'Value':it[1]},ignore_index=True)\n",
      "print d\n",
      "%sql postgres@routing select * from seamless limit 1"
     ],
     "language": "python",
     "metadata": {
      "code_folding": [
       0
      ],
      "run_control": {
       "read_only": false
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "DB -- seamless\n",
        "                         Value                                     Var\n",
        "0   2014-10-18 05:00:45.597487                   Oldest SL Link Update\n",
        "1   2014-11-25 20:23:58.846481                   Newest SL Link Update\n",
        "2   2014-10-18 05:00:45.597487           Oldest SL Vend Content Update\n",
        "3   2014-11-25 20:23:58.846481           Newest SL Vend Content Update\n",
        "4                         3169                   # of Seamless Vendors\n",
        "5                            0                # of Duplicate VEND Rows\n",
        "6                      100.0 %  % of Updated Seamless Vendors (\"VEND\")\n",
        "7                            0         # of VEND without Links Updated\n",
        "8                            0       # of VEND without Content Updated\n",
        "9                      93.09 %               % of Active Seamless VEND\n",
        "10                         219             # of Inactive Seamless VEND\n",
        "11                     100.0 %                     % of VEND with geom\n",
        "12                           0                          # WITHOUT geom\n",
        "13                      94.6 %                      % of VEND with BBL\n",
        "14                         171                           # WITHOUT BBL\n",
        "1 rows affected.\n"
       ]
      },
      {
       "html": [
        "<table>\n",
        "    <tr>\n",
        "        <th>sl_link</th>\n",
        "        <th>vend_id</th>\n",
        "        <th>vend_name</th>\n",
        "        <th>description</th>\n",
        "        <th>cuisine</th>\n",
        "        <th>catering</th>\n",
        "        <th>address</th>\n",
        "        <th>zipcode</th>\n",
        "        <th>phone</th>\n",
        "        <th>price</th>\n",
        "        <th>rating</th>\n",
        "        <th>rating_total</th>\n",
        "        <th>rating_perc</th>\n",
        "        <th>reviews</th>\n",
        "        <th>pickup_est</th>\n",
        "        <th>deliv_est</th>\n",
        "        <th>deliv_min</th>\n",
        "        <th>deliv_fee</th>\n",
        "        <th>perc_fee</th>\n",
        "        <th>search_link_blob</th>\n",
        "        <th>estimates_blob</th>\n",
        "        <th>upd_search_links</th>\n",
        "        <th>upd_vend_content</th>\n",
        "        <th>inactive</th>\n",
        "        <th>camis</th>\n",
        "        <th>bbl</th>\n",
        "        <th>lot_cnt</th>\n",
        "        <th>geom</th>\n",
        "        <th>id</th>\n",
        "        <th>norm_addr</th>\n",
        "        <th>vent_cnt_per_lot</th>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td>http://www.seamless.com/food-delivery/pazza-notte-formerly-pasta-la-vista-new-york-city.684.r</td>\n",
        "        <td>684</td>\n",
        "        <td>Pazza Notte</td>\n",
        "        <td>Italian cuisine with a wide variety of seafood dishes, pasta and pizza available.</td>\n",
        "        <td>Italian</td>\n",
        "        <td>NaN</td>\n",
        "        <td>1375a Avenue Of The Americas</td>\n",
        "        <td>10019</td>\n",
        "        <td>2127656288</td>\n",
        "        <td>5.0</td>\n",
        "        <td>4.0</td>\n",
        "        <td>50</td>\n",
        "        <td>12.0</td>\n",
        "        <td>14.0</td>\n",
        "        <td>10-15</td>\n",
        "        <td>25-40</td>\n",
        "        <td>15.0</td>\n",
        "        <td>None</td>\n",
        "        <td>1</td>\n",
        "        <td>&lt;span class='tooltip-v-name'&gt;Pazza Notte&lt;/span&gt;1375A AVENUE OF THE AMERICAS&lt;br /&gt;&lt;br /&gt;&lt;em&gt;Italian cuisine with a wide variety of seafood dishes, pasta and pizza available.&lt;/em&gt;&lt;br /&gt;&lt;br /&gt;&lt;div&gt;Open on Seamless:\u00a0&lt;strong&gt;11:30 AM - 10:15 PM&lt;/strong&gt;&lt;/div&gt;</td>\n",
        "        <td>Delivery Minimum: $15.00 </td>\n",
        "        <td>2014-11-18 19:25:56.666174-05:00</td>\n",
        "        <td>2014-11-18 19:25:56.666174-05:00</td>\n",
        "        <td>False</td>\n",
        "        <td>40843676</td>\n",
        "        <td>None</td>\n",
        "        <td>0</td>\n",
        "        <td>0101000020E610000013831B84947E52C07BC96BCABA614440</td>\n",
        "        <td>3237</td>\n",
        "        <td>1375a 6 ave</td>\n",
        "        <td>2</td>\n",
        "    </tr>\n",
        "</table>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 4,
       "text": [
        "[(u'http://www.seamless.com/food-delivery/pazza-notte-formerly-pasta-la-vista-new-york-city.684.r', 684L, u'Pazza Notte', u'Italian cuisine with a wide variety of seafood dishes, pasta and pizza available.', u'Italian', u'NaN', u'1375a Avenue Of The Americas', 10019, 2127656288L, 5.0, 4.0, 50, 12.0, 14.0, u'10-15', u'25-40', 15.0, None, 1L, u\"<span class='tooltip-v-name'>Pazza Notte</span>1375A AVENUE OF THE AMERICAS<br /><br /><em>Italian cuisine with a wide variety of seafood dishes, pasta and pizza available.</em><br /><br /><div>Open on Seamless:\\xa0<strong>11:30 AM - 10:15 PM</strong></div>\", u'Delivery Minimum: $15.00 ', datetime.datetime(2014, 11, 18, 19, 25, 56, 666174, tzinfo=psycopg2.tz.FixedOffsetTimezone(offset=-300, name=None)), datetime.datetime(2014, 11, 18, 19, 25, 56, 666174, tzinfo=psycopg2.tz.FixedOffsetTimezone(offset=-300, name=None)), False, 40843676L, None, 0, '0101000020E610000013831B84947E52C07BC96BCABA614440', 3237, u'1375a 6 ave', 2)]"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "# seamless_closed analysis\n",
      "print 'DB -- seamless_closed'\n",
      "d = pd.DataFrame(columns=['Value','Var'])\n",
      "\n",
      "d = d.append({'Var':'# of Potential Active Vendors to Scrape',\n",
      "              'Value':pd.read_sql(\"\"\" \n",
      "                                    select count(*) u from seamless_closed sc\n",
      "                                    where inactive is False\n",
      "                                    and upd_seamless is false\n",
      "                                    \"\"\",engine).u[0]},\n",
      "            ignore_index=True)\n",
      "\n",
      "d = d.append({'Var':'# of Potential Vendors Matched to seamless DB',\n",
      "              'Value':pd.read_sql(\"\"\" \n",
      "                                    select count(*) u from seamless_closed sc\n",
      "                                    inner join seamless s\n",
      "                                    on s.sl_link ilike sc.sl_link\n",
      "                                    \"\"\",engine).u[0]},\n",
      "            ignore_index=True)\n",
      "\n",
      "d = d.append({'Var':'(above) NOT Matched to seamless DB',\n",
      "              'Value':pd.read_sql(\"\"\" \n",
      "                                    select count(*) u from seamless_closed sc\n",
      "                                    where not exists \n",
      "                                        (select * from seamless s \n",
      "                                        where s.sl_link = sc.sl_link)\n",
      "                                    and inactive is False\n",
      "                                    \"\"\",engine).u[0]},\n",
      "            ignore_index=True)\n",
      "\n",
      "print d"
     ],
     "language": "python",
     "metadata": {
      "code_folding": [
       0
      ],
      "run_control": {
       "read_only": false
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "DB -- seamless_closed\n",
        "   Value                                            Var\n",
        "0      0        # of Potential Active Vendors to Scrape\n",
        "1      0  # of Potential Vendors Matched to seamless DB\n",
        "2      0             (above) NOT Matched to seamless DB\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "# mnv analysis\n",
      "\n",
      "# CLEAN UP\n",
      "\n",
      "    # 1. remove entries outside of Manhattan\n",
      "\n",
      "# conn.set_isolation_level(0)\n",
      "# cur.execute(delete_cmd)\n",
      "\n",
      "cmd = \"\"\"\n",
      "select s.* from mnv s, (select array_agg(B::integer) A from unnest(array%(mn_zipcodes)s) B) as f1 \n",
      "where not array[s.zipcode] && A order by s.zipcode;\"\"\"%{'mn_zipcodes':mn_zipcodes}\n",
      "test = pd.read_sql(cmd,engine)\n",
      "if len(test)>0:\n",
      "    print \"\"\"Really delete out-of-area rows?\\n\n",
      "             Use This:\\n\\n\n",
      "             conn.set_isolation_level(0)\n",
      "             cur.execute(delete_cmd)\\n\"\"\"\n",
      "    delete_cmd = \"\"\"\n",
      "        delete from mnv s\n",
      "        using (select array_agg(B::integer) A from unnest(array%(mn_zipcodes)s) B) as f1 \n",
      "        where not array[s.zipcode] && A;\"\"\"%{'mn_zipcodes':mn_zipcodes}\n",
      "    raise SystemError()\n",
      "\n",
      "print 'DB -- mnv (Manhattan Vendors)'\n",
      "d = pd.DataFrame(columns=['Value','Var'])\n",
      "\n",
      "mn = pd.read_sql('select * from mnv',engine)\n",
      "mn['phone'] = mn.phone.map(lambda s: 0 if str(s).lower()=='nan' else int(s))\n",
      "mn_vend_tot = len(mn)#len(mn.camis.unique().tolist())\n",
      "\n",
      "print '\\nRecords from \"mn_vendor\" are limited to the last 18 months from now\\n'\n",
      "back_18_months = dt.now() - DateOffset(months=18)\n",
      "mn_a = mn[mn.inspdate>back_18_months]\n",
      "mn_a_vend_tot = mn_rec_within_18_months = len(mn_a)#len(mn_a.camis.unique().tolist())\n",
      "\n",
      "mn_perc_active_within_18_months = str(round(100 * (float(mn_a_vend_tot)/float(mn_vend_tot)),2))+' %'\n",
      "mn_INACTIVE_within_18_months = mn_vend_tot - mn_a_vend_tot\n",
      "\n",
      "mn_a_with_address = len(mn_a[mn_a.address.isnull()==False])\n",
      "mn_a_WITHOUT_address = mn_a_vend_tot - mn_a_with_address\n",
      "mn_a_perc_with_address = str(round(100 * (float(mn_a_with_address)/float(mn_a_vend_tot)),2))+' %'\n",
      "\n",
      "mn_a_with_geom = len(mn_a[mn_a.geom.isnull()==False])\n",
      "mn_a_WITHOUT_geom = mn_a_vend_tot - mn_a_with_geom\n",
      "mn_a_perc_with_geom = str(round(100 * (float(mn_a_with_geom)/float(mn_a_vend_tot)),2))+' %'\n",
      "\n",
      "mn_a_yelp_matched = len(mn_a[mn_a.yelp_id.isnull()==False])\n",
      "mn_a_WITHOUT_yelp_match = mn_a_vend_tot - mn_a_yelp_matched\n",
      "mn_a_perc_with_yelp_match = str(round(100 * (float(mn_a_yelp_matched)/float(mn_a_vend_tot)),2))+' %'\n",
      "\n",
      "mn_a_seamless_matched = len(mn_a[mn_a.seamless_id.isnull()==False])\n",
      "mn_a_WITHOUT_seamless_match = mn_a_vend_tot - mn_a_seamless_matched\n",
      "mn_a_perc_with_seamless_match = str(round(100 * (float(mn_a_seamless_matched)/float(mn_a_vend_tot)),2))+' %'\n",
      "\n",
      "##### [ insert ]\n",
      "\n",
      "    ### Get Info Per Group of All Records:\n",
      "    #\n",
      "    # most_recent_rec = mn[mn.inspdate>back_18_months].sort('inspdate',ascending=False).inspdate.tolist()[0]\n",
      "    # g = mn[mn.inspdate>back_18_months].groupby('camis')\n",
      "    # g_keys = g.groups.keys()\n",
      "    # g_df = pd.DataFrame(columns=['camis','most_recent_rec'])\n",
      "    # for it in g_keys:\n",
      "    #     t = g.get_group(it).sort('inspdate',ascending=False)\n",
      "    #     t_camis = t.camis.tolist()[0]\n",
      "    #     t_most_recent_rec = t.inspdate.tolist()[0]\n",
      "    #     g_df = g_df.append({'camis':t_camis,'most_recent_rec':t_latest_rec},ignore_index=True)\n",
      "    # mn_vend_inspected_within_18_months = len(g_df)\n",
      "    #\n",
      "    ###\n",
      "\n",
      "\n",
      "    # SL_phone_matches = pd.read_sql(\"\"\" \n",
      "    #                                     select count(*) u from seamless s\n",
      "    #                                     inner join mn_vendors mnv \n",
      "    #                                     on mnv.phone = s.phone\n",
      "    #                                     where mnv.phone is not null\n",
      "    #                                     \"\"\",engine).u[0]\n",
      "    # SL_vend_total = pd.read_sql(\"select count(*) u from seamless\",engine).u[0]\n",
      "    # d = d.append({'Var':'% of Seamless having matching Phone',\n",
      "    #               'Value': str(round(100 * (float(SL_phone_matches)/float(SL_vend_total)),2))+' %'},\n",
      "    #             ignore_index=True)    \n",
      "    # if SL_phone_matches!=SL_vend_total:\n",
      "    #     d = d.append({'Var':'# WITHOUT matching Phone',\n",
      "    #                   'Value':SL_vend_total - SL_phone_matches},\n",
      "    #                 ignore_index=True)\n",
      "\n",
      "    # yelp_phone_matches = pd.read_sql(\"\"\" \n",
      "    #                                     select count(*) u from yelp s\n",
      "    #                                     inner join mn_vendors mnv \n",
      "    #                                     on mnv.phone = s.phone\n",
      "    #                                     where mnv.phone is not null\n",
      "    #                                     \"\"\",engine).u[0]\n",
      "    # yelp_vend_total = pd.read_sql(\"select count(*) u from yelp\",engine).u[0]\n",
      "    # d = d.append({'Var':'% of Yelp having matching Phone',\n",
      "    #               'Value': str(round(100 * (float(yelp_phone_matches)/float(yelp_vend_total)),2))+' %'},\n",
      "    #             ignore_index=True)    \n",
      "    # if yelp_phone_matches!=yelp_vend_total:\n",
      "    #     d = d.append({'Var':'# WITHOUT matching Phone',\n",
      "    #                   'Value':yelp_vend_total - yelp_phone_matches},\n",
      "    #                 ignore_index=True)\n",
      "\n",
      "\n",
      "\n",
      "    # d = d.append({'Var':'(above) Matched with NYC Tax Lots',\n",
      "    #               'Value':pd.read_sql(\"\"\" \n",
      "    #                                     select count(*) u from seamless s\n",
      "    #                                     inner join mn_vendors mnv on to_char(mnv.phone::numeric,'9999999999'::text)::bigint = s.phone\n",
      "    #                                     where mnv.phone is not null\n",
      "    #                                     and s.bbl is not null\n",
      "    #                                     \"\"\",engine).u[0]},\n",
      "    #             ignore_index=True)\n",
      "\n",
      "    # d = d.append({'Var':'Unique Tax Lots: [ lot_pts - mn_vendor - SL ]',\n",
      "    #               'Value': '%s - %s - %s'% (str(pd.read_sql(\"\"\"select count(distinct bbl) u from lot_pts\"\"\",engine).u[0]),\n",
      "    #                                    str(pd.read_sql(\"\"\"select count(distinct bbl) u from mn_vendors\"\"\",engine).u[0]),\n",
      "    #                                    str(pd.read_sql(\"\"\"select count(distinct bbl) u from seamless\"\"\",engine).u[0])) \n",
      "    #                 },\n",
      "    #             ignore_index=True)\n",
      "\n",
      "    # d = d.append({'Var':'# of Active Seamless Vendors',\n",
      "    #               'Value':pd.read_sql(\"\"\" \n",
      "    #                                     select count(*) u \n",
      "    #                                     from seamless \n",
      "    #                                     where inactive is false\n",
      "    #                                     \"\"\",engine).u[0]},\n",
      "    #             ignore_index=True)\n",
      "\n",
      "mn_a_with_bbl = mn_a.bbl.map(lambda s: s.is_integer()).tolist().count(True)\n",
      "mn_a_perc_with_bbl = str(round(100 * (float(mn_a_with_bbl)/float(mn_a_vend_tot)),2))+' %'\n",
      "mn_a_WITHOUT_bbl = mn_a_vend_tot - mn_a_with_bbl\n",
      "\n",
      "mn_a_with_10_digit_phone = mn_a.phone.map(lambda s: len(str(s))==10).tolist().count(True)\n",
      "mn_a_WITHOUT_10_digit_phone = mn_a.phone.map(lambda s: len(str(s))==10).tolist().count(False)\n",
      "mn_a_perc_with_10_digit_phone = str(round(100 * (float(mn_a_with_10_digit_phone)/float(mn_a_vend_tot)),2))+' %'\n",
      "\n",
      "params = [\n",
      "    ('# of NYC Restaurants (\"Vend.\")',len(mn)),\n",
      "    ('% of Vend. records active < 18 months (\"VEND\")',mn_perc_active_within_18_months),\n",
      "    ('# of VEND',len(mn_a)),\n",
      "    ('% of VEND with address',mn_a_perc_with_address),\n",
      "    ('# WITHOUT address',mn_a_WITHOUT_address),\n",
      "    ('% of VEND with geom',mn_a_perc_with_geom),\n",
      "    ('# WITHOUT geom',mn_a_WITHOUT_geom),\n",
      "    ('% of VEND with BBL',mn_a_perc_with_bbl),\n",
      "    ('# WITHOUT BBL',mn_a_WITHOUT_bbl),\n",
      "    ('% of VEND with 10-digit Phone',mn_a_perc_with_10_digit_phone),\n",
      "    ('# WITHOUT 10-digit Phone',mn_a_WITHOUT_10_digit_phone),\n",
      "    ('% of VEND matched with Yelp',mn_a_perc_with_yelp_match),\n",
      "    ('# NOT matched with Yelp',mn_a_WITHOUT_yelp_match),\n",
      "    ('% of VEND matched with Seamless',mn_a_perc_with_seamless_match),\n",
      "    ('# NOT matched with Seamless',mn_a_WITHOUT_seamless_match),\n",
      "]\n",
      "\n",
      "for it in params:\n",
      "    d = d.append({'Var':it[0],'Value':it[1]},ignore_index=True)\n",
      "\n",
      "print d\n",
      "%sql postgres@routing select * from mnv limit 1"
     ],
     "language": "python",
     "metadata": {
      "code_folding": [
       0
      ],
      "run_control": {
       "read_only": false
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "DB -- mnv (Manhattan Vendors)\n",
        "\n",
        "Records from \"mn_vendor\" are limited to the last 18 months from now\n",
        "\n",
        "      Value                                             Var\n",
        "0     10195                  # of NYC Restaurants (\"Vend.\")\n",
        "1   96.85 %  % of Vend. records active < 18 months (\"VEND\")\n",
        "2      9874                                       # of VEND\n",
        "3   99.86 %                          % of VEND with address\n",
        "4        14                               # WITHOUT address\n",
        "5   92.85 %                             % of VEND with geom\n",
        "6       706                                  # WITHOUT geom\n",
        "7    89.4 %                              % of VEND with BBL\n",
        "8      1047                                   # WITHOUT BBL\n",
        "9   99.95 %                   % of VEND with 10-digit Phone\n",
        "10        5                        # WITHOUT 10-digit Phone\n",
        "11  26.84 %                     % of VEND matched with Yelp\n",
        "12     7224                         # NOT matched with Yelp\n",
        "13  20.26 %                 % of VEND matched with Seamless\n",
        "14     7874                     # NOT matched with Seamless\n",
        "1 rows affected.\n"
       ]
      },
      {
       "html": [
        "<table>\n",
        "    <tr>\n",
        "        <th>building</th>\n",
        "        <th>camis</th>\n",
        "        <th>vend_name</th>\n",
        "        <th>clean_street</th>\n",
        "        <th>cuisinecode</th>\n",
        "        <th>dba</th>\n",
        "        <th>inspdate</th>\n",
        "        <th>street</th>\n",
        "        <th>id</th>\n",
        "        <th>recog_street</th>\n",
        "        <th>recog_addr</th>\n",
        "        <th>bbl</th>\n",
        "        <th>phone</th>\n",
        "        <th>norm_addr</th>\n",
        "        <th>seamless_id</th>\n",
        "        <th>yelp_id</th>\n",
        "        <th>address</th>\n",
        "        <th>geom</th>\n",
        "        <th>lot_cnt</th>\n",
        "        <th>zipcode</th>\n",
        "        <th>cuisine_description</th>\n",
        "        <th>grade</th>\n",
        "        <th>grade_date</th>\n",
        "        <th>inspection_type</th>\n",
        "        <th>record_date</th>\n",
        "        <th>violation_code</th>\n",
        "        <th>violation_description</th>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td>464</td>\n",
        "        <td>40363644</td>\n",
        "        <td>dominos pizza</td>\n",
        "        <td>3 ave</td>\n",
        "        <td>None</td>\n",
        "        <td>DOMINO'S PIZZA</td>\n",
        "        <td>2014-08-05 00:00:00</td>\n",
        "        <td>3 AVENUE</td>\n",
        "        <td>22250</td>\n",
        "        <td>None</td>\n",
        "        <td>False</td>\n",
        "        <td>None</td>\n",
        "        <td>2125450200</td>\n",
        "        <td>None</td>\n",
        "        <td>None</td>\n",
        "        <td>None</td>\n",
        "        <td>None</td>\n",
        "        <td>None</td>\n",
        "        <td>1</td>\n",
        "        <td>10016</td>\n",
        "        <td>Pizza</td>\n",
        "        <td>A</td>\n",
        "        <td>2014-08-05T00:00:00</td>\n",
        "        <td>Cycle Inspection / Initial Inspection</td>\n",
        "        <td>2014-11-18T06:01:25</td>\n",
        "        <td>10F</td>\n",
        "        <td>Non-food contact surface improperly constructed. Unacceptable material used. Non-food contact surface or equipment improperly maintained and/or not properly sealed, raised, spaced or movable to allow accessibility for cleaning on all sides, above and underneath the unit.</td>\n",
        "    </tr>\n",
        "</table>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 45,
       "text": [
        "[(u'464', 40363644L, u'dominos pizza', u'3 ave', None, u\"DOMINO'S PIZZA\", datetime.datetime(2014, 8, 5, 0, 0), u'3 AVENUE', 22250, None, False, None, 2125450200L, None, None, None, None, None, 1, 10016, u'Pizza', u'A', u'2014-08-05T00:00:00', u'Cycle Inspection / Initial Inspection', u'2014-11-18T06:01:25', u'10F', u'Non-food contact surface improperly constructed. Unacceptable material used. Non-food contact surface or equipment improperly maintained and/or not properly sealed, raised, spaced or movable to allow accessibility for cleaning on all sides, above and underneath the unit.')]"
       ]
      }
     ],
     "prompt_number": 45
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "# yelp\n",
      "\n",
      "## CLEANUP FUNCTIONS:\n",
      "\n",
      "# 1. remove entries outside of Manhattan\n",
      "conn.set_isolation_level(0)\n",
      "cur.execute('drop table if exists tmp;')\n",
      "conn.set_isolation_level(0)\n",
      "cur.execute(\"\"\"\n",
      "    create table tmp as\n",
      "    select \n",
      "        s.*\n",
      "    from \n",
      "        yelp s, \n",
      "        (select array_agg(B::bigint) A from unnest(array%(mn_zipcodes)s) B) as f1 \n",
      "    where not array[s.postal_code] && A order by s.postal_code;\n",
      "\n",
      "    delete from yelp y\n",
      "    using tmp t\n",
      "    where t.uid = y.uid;\n",
      "\n",
      "    with upd as (\n",
      "        update yelp_geom_error y\n",
      "        set\n",
      "            id = y.id,\n",
      "            vend_name = y.vend_name,\n",
      "            phone = y.phone,\n",
      "            address = y.address,\n",
      "            city = y.city,\n",
      "            state_code = y.state_code,\n",
      "            postal_code = y.postal_code,\n",
      "            display_phone = y.display_phone,\n",
      "            is_claimed = y.is_claimed,\n",
      "            is_closed = y.is_closed,\n",
      "            menu_date_updated = y.menu_date_updated,\n",
      "            menu_provider = y.menu_provider,\n",
      "            rating = y.rating,\n",
      "            review_count = y.review_count,\n",
      "            categories = y.categories,\n",
      "            url = y.url,\n",
      "            latitude = y.latitude,\n",
      "            longitude = y.longitude,\n",
      "            geo_accuracy = y.geo_accuracy,\n",
      "            hours = y.hours,\n",
      "            hours_updated = y.hours_updated,\n",
      "            extra_info = y.extra_info,\n",
      "            menu_page = y.menu_page,\n",
      "            website = y.website,\n",
      "            online_ordering = y.online_ordering,\n",
      "            price_range = y.price_range,\n",
      "            uid = y.uid,\n",
      "            geom = y.geom,\n",
      "            bbl = y.bbl,\n",
      "            camis = y.camis,\n",
      "            lot_cnt = y.lot_cnt,\n",
      "            norm_addr = y.norm_addr,\n",
      "            last_api_update = y.last_api_update,\n",
      "            neighborhoods = y.neighborhoods,\n",
      "            display_address = y.display_address,\n",
      "            non_mn_zipcode = y.non_mn_zipcode,\n",
      "            last_updated = 'now'::timestamp with time zone\n",
      "        from tmp t\n",
      "        where y.uid = t.uid\n",
      "        returning t.uid uid\n",
      "    )\n",
      "    insert into yelp_geom_error ( \n",
      "            id,\n",
      "            vend_name,\n",
      "            phone,\n",
      "            address,\n",
      "            city,\n",
      "            state_code,\n",
      "            postal_code,\n",
      "            display_phone,\n",
      "            is_claimed,\n",
      "            is_closed,\n",
      "            menu_date_updated,\n",
      "            menu_provider,\n",
      "            rating,\n",
      "            review_count,\n",
      "            categories,\n",
      "            url,\n",
      "            latitude,\n",
      "            longitude,\n",
      "            geo_accuracy,\n",
      "            hours,\n",
      "            hours_updated,\n",
      "            extra_info,\n",
      "            menu_page,\n",
      "            website,\n",
      "            online_ordering,\n",
      "            price_range,\n",
      "            uid,\n",
      "            geom,\n",
      "            bbl,\n",
      "            camis,\n",
      "            lot_cnt,\n",
      "            norm_addr,\n",
      "            last_api_update,\n",
      "            neighborhoods,\n",
      "            display_address,\n",
      "            non_mn_zipcode,\n",
      "            last_updated)\n",
      "    select\n",
      "        t.id,\n",
      "        t.vend_name,\n",
      "        t.phone,\n",
      "        t.address,\n",
      "        t.city,\n",
      "        t.state_code,\n",
      "        t.postal_code,\n",
      "        t.display_phone,\n",
      "        t.is_claimed,\n",
      "        t.is_closed,\n",
      "        t.menu_date_updated,\n",
      "        t.menu_provider,\n",
      "        t.rating,\n",
      "        t.review_count,\n",
      "        t.categories,\n",
      "        t.url,\n",
      "        t.latitude,\n",
      "        t.longitude,\n",
      "        t.geo_accuracy,\n",
      "        t.hours,\n",
      "        t.hours_updated,\n",
      "        t.extra_info,\n",
      "        t.menu_page,\n",
      "        t.website,\n",
      "        t.online_ordering,\n",
      "        t.price_range,\n",
      "        t.uid,\n",
      "        t.geom,\n",
      "        t.bbl,\n",
      "        t.camis,\n",
      "        t.lot_cnt,\n",
      "        t.norm_addr,\n",
      "        t.last_api_update,\n",
      "        t.neighborhoods,\n",
      "        t.display_address,\n",
      "        t.non_mn_zipcode,\n",
      "        'now'::timestamp with time zone now\n",
      "    from\n",
      "        tmp t,\n",
      "        (select array_agg(f.uid) upd_uids from upd f) as f1\n",
      "    where (not upd_uids && array[t.uid]\n",
      "        or upd_uids is null);\n",
      "    \n",
      "    drop table tmp;\n",
      "            \"\"\"%{'mn_zipcodes':mn_zipcodes})\n",
      "\n",
      "\n",
      "print 'DB -- yelp'\n",
      "d = pd.DataFrame(columns=['Value','Var'])\n",
      "\n",
      "y = pd.read_sql('select * from yelp',engine)\n",
      "y_vend_cnt = len(y)\n",
      "y_a = y[y.is_closed==False]\n",
      "y_a_vend_cnt = len(y_a)\n",
      "y_a_uniq_vend = len(y_a.id.unique().tolist())\n",
      "\n",
      "oldest_search_api_query = y_a.sort('last_api_update',ascending=True).last_api_update.tolist()[0]\n",
      "newest_search_api_query = y_a.sort('last_api_update',ascending=False).last_api_update.tolist()[0]\n",
      "oldest_y_page_scrape = y_a.sort('hours_updated',ascending=True).hours_updated.tolist()[0]\n",
      "newest_y_page_scrape = y_a.sort('hours_updated',ascending=False).hours_updated.tolist()[0]\n",
      "\n",
      "y_a_with_info = len(y_a[y_a.hours.map(lambda s: True if len(str(s))!=0 else False)==True])\n",
      "y_a_perc_with_info = str(round(100 * (float(y_a_with_info)/float(y_a_vend_cnt)),2))+' %'\n",
      "y_a_WITHOUT_info = y_a_vend_cnt - y_a_with_info\n",
      "\n",
      "y_a_with_geom = len(y_a[y_a.geom.isnull()==False])\n",
      "y_a_WITHOUT_geom = y_a_vend_cnt - y_a_with_geom\n",
      "y_a_perc_with_geom = str(round(100 * (float(y_a_with_geom)/float(y_a_vend_cnt)),2))+' %'\n",
      "\n",
      "y_a_with_matched_bbl = y_a.bbl.map(lambda s: True if str(s).lower()!='nan' else False).tolist().count(True)\n",
      "y_a_perc_with_matched_bbl = str(round(100 * (float(y_a_with_matched_bbl)/float(y_a_vend_cnt)),2))+' %'\n",
      "y_a_WITHOUT_matched_bbl = y_a_vend_cnt - y_a_with_matched_bbl\n",
      "\n",
      "y_a_claiming_pages = len(y_a[y_a.is_claimed==True])\n",
      "y_a_NOT_claiming_pages = y_a_vend_cnt - y_a_claiming_pages\n",
      "y_a_perc_claiming_pages = str(round(100 * (float(y_a_claiming_pages)/float(y_a_vend_cnt)),2))+' %'\n",
      "\n",
      "y_a_with_online_ordering = len(y_a[y_a.online_ordering==True])\n",
      "y_a_WITHOUT_online_ordering = y_a_vend_cnt - y_a_with_online_ordering\n",
      "y_a_perc_online_ordering = str(round(100 * (float(y_a_with_online_ordering)/float(y_a_vend_cnt)),2))+' %'\n",
      "\n",
      "params = [\n",
      "    ('Oldest Search API Query',oldest_search_api_query),\n",
      "    ('Latest Search API Query',newest_search_api_query),\n",
      "    ('Oldest Biz Page Scrape',oldest_y_page_scrape),\n",
      "    ('Latest Biz Page Scrape',newest_y_page_scrape),\n",
      "    ('# of Total Yelp Vendors',y_vend_cnt),\n",
      "    ('# of Active Yelp Vendors (\"VEND\")',y_a_vend_cnt),\n",
      "    ('% of VEND with all info obtained',y_a_perc_with_info),\n",
      "    ('# WITHOUT all info obtained',y_a_WITHOUT_info),\n",
      "    ('% of VEND with geom',y_a_perc_with_geom),\n",
      "    ('# WITHOUT geom',y_a_WITHOUT_geom),\n",
      "    ('% of VEND Matched BBL',y_a_perc_with_matched_bbl),\n",
      "    ('# NOT Matched BBL',y_a_WITHOUT_matched_bbl),\n",
      "    ('% of VEND Claiming Pages',y_a_perc_claiming_pages),\n",
      "    ('# NOT Claiming Pages',y_a_NOT_claiming_pages),\n",
      "    ('% of VEND with Online Ordering',y_a_perc_online_ordering),\n",
      "    ('# WITHOUT Online Ordering',y_a_WITHOUT_online_ordering),\n",
      "         ]\n",
      "\n",
      "for it in params:\n",
      "    d = d.append({'Var':it[0],'Value':it[1]},ignore_index=True)\n",
      "\n",
      "print d\n",
      "%sql postgres@routing select * from yelp limit 1"
     ],
     "language": "python",
     "metadata": {
      "code_folding": [
       0,
       103
      ],
      "run_control": {
       "read_only": false
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "DB -- yelp\n",
        "                         Value                                Var\n",
        "0   2014-11-17 21:51:30.806388            Oldest Search API Query\n",
        "1   2014-11-24 23:44:45.301603            Latest Search API Query\n",
        "2   2014-11-04 15:31:44.994299             Oldest Biz Page Scrape\n",
        "3   2014-11-25 15:18:53.462294             Latest Biz Page Scrape\n",
        "4                         4015            # of Total Yelp Vendors\n",
        "5                         4015  # of Active Yelp Vendors (\"VEND\")\n",
        "6                      100.0 %   % of VEND with all info obtained\n",
        "7                            0        # WITHOUT all info obtained\n",
        "8                      94.57 %                % of VEND with geom\n",
        "9                          218                     # WITHOUT geom\n",
        "10                      87.8 %              % of VEND Matched BBL\n",
        "11                         490                  # NOT Matched BBL\n",
        "12                     59.53 %           % of VEND Claiming Pages\n",
        "13                        1625               # NOT Claiming Pages\n",
        "14                     23.31 %     % of VEND with Online Ordering\n",
        "15                        3079          # WITHOUT Online Ordering\n",
        "1 rows affected.\n"
       ]
      },
      {
       "html": [
        "<table>\n",
        "    <tr>\n",
        "        <th>id</th>\n",
        "        <th>vend_name</th>\n",
        "        <th>phone</th>\n",
        "        <th>address</th>\n",
        "        <th>city</th>\n",
        "        <th>state_code</th>\n",
        "        <th>postal_code</th>\n",
        "        <th>display_phone</th>\n",
        "        <th>is_claimed</th>\n",
        "        <th>is_closed</th>\n",
        "        <th>menu_date_updated</th>\n",
        "        <th>menu_provider</th>\n",
        "        <th>rating</th>\n",
        "        <th>review_count</th>\n",
        "        <th>categories</th>\n",
        "        <th>url</th>\n",
        "        <th>latitude</th>\n",
        "        <th>longitude</th>\n",
        "        <th>geo_accuracy</th>\n",
        "        <th>hours</th>\n",
        "        <th>hours_updated</th>\n",
        "        <th>extra_info</th>\n",
        "        <th>menu_page</th>\n",
        "        <th>website</th>\n",
        "        <th>online_ordering</th>\n",
        "        <th>price_range</th>\n",
        "        <th>geom</th>\n",
        "        <th>bbl</th>\n",
        "        <th>camis</th>\n",
        "        <th>lot_cnt</th>\n",
        "        <th>norm_addr</th>\n",
        "        <th>last_api_update</th>\n",
        "        <th>neighborhoods</th>\n",
        "        <th>display_address</th>\n",
        "        <th>upd_search_links</th>\n",
        "        <th>phone_as_text</th>\n",
        "        <th>non_mn_zipcode</th>\n",
        "        <th>last_updated</th>\n",
        "        <th>vend_cnt</th>\n",
        "        <th>uid</th>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td>caf%C3%A9-ollin-new-york-3</td>\n",
        "        <td>Caf\u00e9 Ollin</td>\n",
        "        <td>2128283644</td>\n",
        "        <td>339 E 108th St</td>\n",
        "        <td>New York</td>\n",
        "        <td>NY</td>\n",
        "        <td>10029</td>\n",
        "        <td>+1-212-828-3644</td>\n",
        "        <td>True</td>\n",
        "        <td>False</td>\n",
        "        <td>2014-11-18 03:51:54</td>\n",
        "        <td>single_platform</td>\n",
        "        <td>4.5</td>\n",
        "        <td>227</td>\n",
        "        <td>{{Mexican,mexican},{\"Latin American\",latin}}</td>\n",
        "        <td>http://www.yelp.com/biz/caf%C3%A9-ollin-new-york-3</td>\n",
        "        <td>40.7912288</td>\n",
        "        <td>-73.9395386</td>\n",
        "        <td>9.0</td>\n",
        "        <td>[(\"Mon\", \"10:00 am - 10:00 pm\"), (\"Tue\", \"10:00 am - 10:00 pm\"), (\"Wed\", \"10:00 am - 10:00 pm\"), (\"Thu\", \"10:00 am - 10:00 pm\"), (\"Fri\", \"10:00 am - 10:00 pm\"), (\"Sat\", \"10:00 am - 10:00 pm\"), (\"Sun\", \"10:00 am - 10:00 pm\")]</td>\n",
        "        <td>2014-11-25 09:45:39.642884-05:00</td>\n",
        "        <td>{\"Take-out\": \"Yes\", \"Wi-Fi\": \"No\", \"Alcohol\": \"Beer &amp; Wine Only\", \"Caters\": \"Yes\", \"Noise Level\": \"Average\", \"Outdoor Seating\": \"No\", \"Takes Reservations\": \"Yes\", \"Delivery\": \"Yes\", \"Ambience\": \"Casual\", \"Bike Parking\": \"Yes\", \"Has TV\": \"Yes\", \"Good For\": \"Lunch, Dinner\", \"Parking\": \"Street\", \"Attire\": \"Casual\", \"Waiter Service\": \"Yes\", \"Accepts Credit Cards\": \"Yes\", \"Good for Kids\": \"Yes\", \"Good for Groups\": \"Yes\", \"Wheelchair Accessible\": \"No\"}</td>\n",
        "        <td>None</td>\n",
        "        <td>http://www.cafeollin.com</td>\n",
        "        <td>True</td>\n",
        "        <td>Under $10</td>\n",
        "        <td>None</td>\n",
        "        <td>None</td>\n",
        "        <td>None</td>\n",
        "        <td>0</td>\n",
        "        <td>None</td>\n",
        "        <td>2014-11-24 18:32:50.259889-05:00</td>\n",
        "        <td>East Harlem</td>\n",
        "        <td>339 E 108th St,East Harlem,New York, NY 10029</td>\n",
        "        <td>2014-11-26 00:04:57.782384-05:00</td>\n",
        "        <td>2128283644</td>\n",
        "        <td>False</td>\n",
        "        <td>2014-11-27 01:06:11.745938-05:00</td>\n",
        "        <td>None</td>\n",
        "        <td>3798</td>\n",
        "    </tr>\n",
        "</table>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 56,
       "text": [
        "[(u'caf%C3%A9-ollin-new-york-3', u'Caf\\xe9 Ollin', 2128283644L, u'339 E 108th St', u'New York', u'NY', 10029L, u'+1-212-828-3644', True, False, u'2014-11-18 03:51:54', u'single_platform', 4.5, 227L, u'{{Mexican,mexican},{\"Latin American\",latin}}', u'http://www.yelp.com/biz/caf%C3%A9-ollin-new-york-3', 40.7912288, -73.9395386, 9.0, u'[(\"Mon\", \"10:00 am - 10:00 pm\"), (\"Tue\", \"10:00 am - 10:00 pm\"), (\"Wed\", \"10:00 am - 10:00 pm\"), (\"Thu\", \"10:00 am - 10:00 pm\"), (\"Fri\", \"10:00 am - 10:00 pm\"), (\"Sat\", \"10:00 am - 10:00 pm\"), (\"Sun\", \"10:00 am - 10:00 pm\")]', datetime.datetime(2014, 11, 25, 9, 45, 39, 642884, tzinfo=psycopg2.tz.FixedOffsetTimezone(offset=-300, name=None)), u'{\"Take-out\": \"Yes\", \"Wi-Fi\": \"No\", \"Alcohol\": \"Beer & Wine Only\", \"Caters\": \"Yes\", \"Noise Level\": \"Average\", \"Outdoor Seating\": \"No\", \"Takes Reservations\": \"Yes\", \"Delivery\": \"Yes\", \"Ambience\": \"Casual\", \"Bike Parking\": \"Yes\", \"Has TV\": \"Yes\", \"Good For\": \"Lunch, Dinner\", \"Parking\": \"Street\", \"Attire\": \"Casual\", \"Waiter Service\": \"Yes\", \"Accepts Credit Cards\": \"Yes\", \"Good for Kids\": \"Yes\", \"Good for Groups\": \"Yes\", \"Wheelchair Accessible\": \"No\"}', u'None', u'http://www.cafeollin.com', True, u'Under $10', None, None, None, 0, None, datetime.datetime(2014, 11, 24, 18, 32, 50, 259889, tzinfo=psycopg2.tz.FixedOffsetTimezone(offset=-300, name=None)), u'East Harlem', u'339 E 108th St,East Harlem,New York, NY 10029', datetime.datetime(2014, 11, 26, 0, 4, 57, 782384, tzinfo=psycopg2.tz.FixedOffsetTimezone(offset=-300, name=None)), u'2128283644', False, datetime.datetime(2014, 11, 27, 1, 6, 11, 745938, tzinfo=psycopg2.tz.FixedOffsetTimezone(offset=-300, name=None)), None, 3798)]"
       ]
      }
     ],
     "prompt_number": 56
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "### yelp: remove unlikely matches by comparing matched/added geoms w/ yelp-provided coord.s\n",
      "cmd=\"\"\"\n",
      "\n",
      "drop table if exists tmp;\n",
      "\n",
      "create table tmp as\n",
      "    SELECT \n",
      "        y.id compare_id,\n",
      "        y.vend_name vend_name,\n",
      "        y.address address,\n",
      "        0.0::double precision seperation_dist,\n",
      "        ST_SetSRID(ST_Point(y.longitude, y.latitude),4326)::geometry(Point,4326) geom\n",
      "    FROM yelp y\n",
      "    WHERE geom is not null;\n",
      "\n",
      "update tmp set seperation_dist = null;\n",
      "\n",
      "update tmp A \n",
      "    set seperation_dist = ST_Distance_Spheroid(A.geom,B.geom,\n",
      "                                'SPHEROID[\"WGS 84\",6378137,298.257223563]') \n",
      "    from yelp B \n",
      "    where \n",
      "        A.compare_id = B.id\n",
      "        and A.geom is not null\n",
      "        and B.geom is not null;\n",
      "\n",
      "\"\"\"\n",
      "%sql postgres@routing $cmd"
     ],
     "language": "python",
     "metadata": {
      "code_folding": [
       0
      ],
      "run_control": {
       "read_only": false
      }
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "### yelp: compare matched added geoms w/ yelp-provided coord.s\n",
      "cmd=\"\"\"\n",
      "\n",
      "drop table if exists tmp;\n",
      "\n",
      "create table tmp as\n",
      "    SELECT \n",
      "        y.id compare_id,\n",
      "        y.vend_name vend_name,\n",
      "        y.address address,\n",
      "        0.0::double precision seperation_dist,\n",
      "        ST_SetSRID(ST_Point(y.longitude, y.latitude),4326)::geometry(Point,4326) geom\n",
      "    FROM yelp y\n",
      "    WHERE geom is not null;\n",
      "\n",
      "update tmp set seperation_dist = null;\n",
      "\n",
      "update tmp A \n",
      "    set seperation_dist = ST_Distance_Spheroid(A.geom,B.geom,\n",
      "                                'SPHEROID[\"WGS 84\",6378137,298.257223563]') \n",
      "    from yelp B \n",
      "    where \n",
      "        A.compare_id = B.id\n",
      "        and A.geom is not null\n",
      "        and B.geom is not null;\n",
      "\n",
      "update tmp set geom=null where seperation_dist>400.0;\n",
      "\n",
      "update yelp y set geom=null\n",
      "from tmp t\n",
      "where t.geom is null and y.id=t.compare_id;\n",
      "\n",
      "drop table tmp;\n",
      "\n",
      "\"\"\"\n",
      "%sql postgres@routing $cmd"
     ],
     "language": "python",
     "metadata": {
      "code_folding": [
       0
      ],
      "run_control": {
       "read_only": false
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Unification"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "# add mnv.camis to yelp/seamless on unique phones\n",
      "conn.set_isolation_level(0)\n",
      "cur.execute(\"\"\"\n",
      "    update yelp y1 set camis=m.camis\n",
      "    from \n",
      "        mnv m,\n",
      "        (\n",
      "            select array_agg(phone) B\n",
      "            from \n",
      "                yelp y,\n",
      "                (select array_agg(s1.phone) A from yelp s1) as f1\n",
      "            where (select sum(case b when y.phone then 1 else 0 end)\n",
      "                         from unnest(A) as dt(b))=1\n",
      "        ) as f2\n",
      "    where m.phone = y1.phone\n",
      "    and B && array[y1.phone]\n",
      "    \"\"\")\n",
      "conn.set_isolation_level(0)\n",
      "cur.execute(\"\"\"\n",
      "    update seamless y1 set camis=m.camis\n",
      "    from \n",
      "        mnv m,\n",
      "        (\n",
      "            select array_agg(phone) B\n",
      "            from \n",
      "                seamless y,\n",
      "                (select array_agg(s1.phone) A from yelp s1) as f1\n",
      "            where (select sum(case b when y.phone then 1 else 0 end)\n",
      "                         from unnest(A) as dt(b))=1\n",
      "        ) as f2\n",
      "    where m.phone = y1.phone\n",
      "    and B && array[y1.phone]\n",
      "    \"\"\")"
     ],
     "language": "python",
     "metadata": {
      "code_folding": [
       0
      ],
      "run_control": {
       "read_only": false
      }
     },
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "# add seamless.vend_id to yelp on unique phones\n",
      "conn.set_isolation_level(0)\n",
      "cur.execute(\"\"\"\n",
      "    update yelp y1 set camis=m.camis\n",
      "    from \n",
      "        mnv m,\n",
      "        (\n",
      "            select array_agg(phone) B\n",
      "            from \n",
      "                yelp y,\n",
      "                (select array_agg(s1.phone) A from yelp s1) as f1\n",
      "            where (select sum(case b when y.phone then 1 else 0 end)\n",
      "                         from unnest(A) as dt(b))=1\n",
      "        ) as f2\n",
      "    where m.phone = y1.phone\n",
      "    and B && array[y1.phone]\n",
      "    \"\"\")"
     ],
     "language": "python",
     "metadata": {
      "code_folding": [
       0
      ],
      "run_control": {
       "read_only": false
      }
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "def match_distinct_mnv_var(tbl='seamless',tbl_id='vend_id',vend_name='vend_name',match_var='vend_name'):\n",
      "    \"\"\"\n",
      "    Matches distinct $tbl variables with distinct mn_vendor vars.\n",
      "    \n",
      "    Usages:\n",
      "    \n",
      "        match_distinct_mnv_var(tbl='yelp',tbl_id='id',vend_name='vend_name',match_var='phone')\n",
      "        match_distinct_mnv_var(tbl='seamless',tbl_id='vend_id',vend_name='vend_name',match_var='vend_name')\n",
      "        match_distinct_mnv_var(tbl='yelp',tbl_id='id',vend_name='vend_name',match_var='address')\n",
      "    \n",
      "    \"\"\"\n",
      "    T = {'tbl':tbl,\n",
      "         'tbl_id':tbl_id,\n",
      "         'vend_name':vend_name,\n",
      "         'match_var':match_var}\n",
      "    \n",
      "    x = pd.read_sql(\"select * from %(tbl)s\"%T,engine)\n",
      "\n",
      "    x_var_tot = x[match_var].tolist()\n",
      "    x[match_var+'_cnt'] = x[match_var].map(lambda i: x_var_tot.count(i))\n",
      "    x_var_id_dict = dict(zip(x[x[match_var+'_cnt']==1][match_var].tolist(),x[x[match_var+'_cnt']==1][tbl_id].tolist()))\n",
      "    x_var_id_dict_keys = x_var_id_dict.keys()\n",
      "\n",
      "    mnv  = pd.read_sql(\"select * from mnv where %(tbl)s_id is null\"%T,engine)\n",
      "    mnv_var_tot = mnv[match_var].tolist()\n",
      "    mnv[match_var+'_cnt'] = mnv[match_var].map(lambda i: mnv_var_tot.count(i))\n",
      "    mnv_x = mnv[ (mnv[match_var].isin(x_var_id_dict_keys)==True)&(mnv[match_var+'_cnt']==1) ].copy()\n",
      "    mnv_x['%(tbl)s_id'%T] = mnv_x[match_var].map(x_var_id_dict)\n",
      "\n",
      "    conn.set_isolation_level(0)\n",
      "    cur.execute( \"\"\"drop table if exists tmp;\"\"\")\n",
      "    mnv_x.to_sql('tmp',engine)\n",
      "    conn.set_isolation_level(0)\n",
      "    cur.execute(\"\"\"\n",
      "        alter table tmp add column tid serial primary key;\n",
      "        update tmp set tid = nextval(pg_get_serial_sequence('tmp','tid'));\n",
      "    \"\"\")\n",
      "    conn.set_isolation_level(0)\n",
      "    cur.execute(\"\"\"\n",
      "        update mnv m\n",
      "        set %(tbl)s_id = t.%(tbl)s_id\n",
      "        from tmp t\n",
      "        where m.camis = t.camis;\n",
      "\n",
      "        drop table tmp;\n",
      "    \"\"\"%T)"
     ],
     "language": "python",
     "metadata": {
      "code_folding": [
       0
      ],
      "run_control": {
       "read_only": false
      }
     },
     "outputs": [],
     "prompt_number": 39
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "match_tables = ['seamless','yelp']\n",
      "match_vars = ['vend_name','phone','norm_addr']\n",
      "\n",
      "it=match_vars[1]\n",
      "match_distinct_mnv_var(tbl='seamless',tbl_id='vend_id',vend_name='vend_name',match_var=it)"
     ],
     "language": "python",
     "metadata": {
      "run_control": {
       "read_only": false
      }
     },
     "outputs": [],
     "prompt_number": 44
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sl_phones = pd.read_sql('select distinct phone p from seamless',engine).p.tolist()\n",
      "y = pd.read_sql('select distinct phone p from yelp',engine)\n",
      "y['match'] = y.p.map(lambda s: True if sl_phones.count(s)>0 else False)\n",
      "y.match.tolist().count(True)"
     ],
     "language": "python",
     "metadata": {
      "run_control": {
       "read_only": false
      }
     },
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 48,
       "text": [
        "1050"
       ]
      }
     ],
     "prompt_number": 48
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "### Visualize Matched Vendors\n",
      "c=\"\"\"\n",
      "select \n",
      "    lower(m.vend_name),\n",
      "    lower(y.vend_name),\n",
      "    levenshtein(lower(m.vend_name),lower(y.vend_name),1,0,4),\n",
      "    difference(lower(m.vend_name),lower(y.vend_name))\n",
      "from mnv m,yelp y\n",
      "where m.yelp_id=y.id\n",
      "order by levenshtein(lower(m.vend_name),lower(y.vend_name),1,0,4) desc\n",
      "\"\"\""
     ],
     "language": "python",
     "metadata": {
      "code_folding": [
       0
      ],
      "run_control": {
       "read_only": false
      }
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "### Between like tables, Tranfer Rows from Source that are not in Destination\n",
      "T = {'src':'yelp_geom_error',\n",
      "     'dest':'yelp',\n",
      "     'id_col':'id',\n",
      "     'uid_col':'uid',\n",
      "     'remove_from_src':True}\n",
      "\n",
      "from_cols = pd.read_sql(\"\"\"   \n",
      "    select column_name cols \n",
      "    from INFORMATION_SCHEMA.COLUMNS \n",
      "    where table_name = '%(src)s'\"\"\"%T,engine).cols.map(str).tolist()\n",
      "to_cols = pd.read_sql(\"\"\"   \n",
      "    select column_name cols \n",
      "    from INFORMATION_SCHEMA.COLUMNS \n",
      "    where table_name = '%(dest)s'\"\"\"%T,engine).cols.map(str).tolist()\n",
      "\n",
      "pop_list=[from_cols.index(T['uid_col'])]\n",
      "for it in from_cols:\n",
      "    if to_cols.count(it)==0:\n",
      "        if it!=T['uid_col']:\n",
      "            pop_list.append(from_cols.index(it))\n",
      "            print 'Not Transfering Column:\\t',it\n",
      "a=sorted(pop_list)\n",
      "a.reverse()\n",
      "for it in a: t=from_cols.pop(it)\n",
      "\n",
      "cols = from_cols\n",
      "T.update({'insert_cols': ','.join(cols),\n",
      "          'select_cols': ','.join(map(lambda s: 't.'+s,cols))})\n",
      "c=\"\"\"                 \n",
      "    insert into %(dest)s ( %(insert_cols)s )\n",
      "    select %(select_cols)s\n",
      "    from \n",
      "        %(src)s t,\n",
      "        (select array_agg(t2.%(id_col)s) A from %(dest)s t2) as f2\n",
      "    where (not A @> array[t.%(id_col)s]) and t.postal_code is null;\n",
      "    \"\"\"%T\n",
      "conn.set_isolation_level(0)\n",
      "cur.execute(c)\n",
      "if T['remove_from_src']==True:\n",
      "    c=\"\"\"                 \n",
      "        delete from %(src)s t \n",
      "            using (select array_agg(t2.%(id_col)s) A from %(dest)s t2) as f1\n",
      "            where t.%(id_col)s = any(A);\n",
      "        \"\"\"%T\n",
      "    conn.set_isolation_level(0)\n",
      "    cur.execute(c)   "
     ],
     "language": "python",
     "metadata": {
      "code_folding": [
       0,
       7
      ],
      "run_control": {
       "read_only": false
      }
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "### __ create/update '*_geom_error' tables for remainders of yelp,seamless,mnv\n",
      "\n",
      "tables = ['yelp','seamless']\n",
      "id_cols = {'yelp':'id','seamless':'vend_id'}\n",
      "\n",
      "c=\"\"\"\n",
      "    select table_name tbl from information_schema.tables \n",
      "    WHERE table_schema='public'\n",
      "    and table_name = ANY(array%s)\n",
      "    \"\"\"%str(map(lambda s: s+'_geom_error',tables))\n",
      "chk = pd.read_sql(c,engine).tbl.tolist()\n",
      "\n",
      "for tbl in tables:\n",
      "    T = {'base_tbl':tbl,'err_tbl':tbl+'_geom_error','id_col':id_cols[tbl]}\n",
      "    if chk.count(T['err_tbl'])==0:\n",
      "        c=\"\"\"\n",
      "            create table %(err_tbl)s as\n",
      "                select * from %(base_tbl)s where geom is null;\n",
      "                \n",
      "            delete from %(base_tbl)s t \n",
      "                using (select array_agg(t2.%(id_col)s) A from %(err_tbl)s t2) \n",
      "                where y.%(id_col)s = any(A);\n",
      "            \"\"\"%T\n",
      "        conn.set_isolation_level(0)\n",
      "        cur.execute(c)\n",
      "    else:\n",
      "        cols = pd.read_sql(\"\"\"   \n",
      "            select column_name cols \n",
      "            from INFORMATION_SCHEMA.COLUMNS \n",
      "            where table_name = '%(base_tbl)s'\"\"\"%T,engine).cols.map(str).tolist()\n",
      "        cols = [it for it in cols if it!='last_updated']\n",
      "        T.update({'upd_defs'   : '\\n'.join(map(lambda s: s+' = t.'+s+',',cols))[:-1],\n",
      "                  'insert_cols': ','.join(cols) })\n",
      "        T.update({'select_cols': ','.join(map(lambda s: 't.'+s,cols))})\n",
      "        conn.set_isolation_level(0)\n",
      "        c=\"\"\"\n",
      "            with upd as (\n",
      "                update %(err_tbl)s s\n",
      "                set %(upd_defs)s\n",
      "                ,last_updated = 'now'::timestamp with time zone\n",
      "                from %(base_tbl)s t\n",
      "                where s.%(id_col)s = t.%(id_col)s\n",
      "                and t.geom is null\n",
      "                returning s.%(id_col)s %(id_col)s\n",
      "                        )                   \n",
      "            insert into %(err_tbl)s \n",
      "                ( \n",
      "                    %(insert_cols)s\n",
      "                    ,last_updated \n",
      "                )\n",
      "\n",
      "            select \n",
      "                %(select_cols)s\n",
      "                ,'now'::timestamp with time zone last_updated\n",
      "            from \n",
      "                %(base_tbl)s t,\n",
      "                (select array_agg(t2.%(id_col)s) A from %(err_tbl)s t2) as f2\n",
      "            where (not A @> array[t.%(id_col)s])\n",
      "            and t.geom is null;\n",
      "\n",
      "            delete from %(base_tbl)s t \n",
      "                using (select array_agg(t2.%(id_col)s) A from %(err_tbl)s t2) as f1\n",
      "                where t.%(id_col)s = any(A);\n",
      "            \"\"\"%T\n",
      "        conn.set_isolation_level(0)\n",
      "        cur.execute(c)\n"
     ],
     "language": "python",
     "metadata": {
      "code_folding": [
       0
      ],
      "run_control": {
       "read_only": false
      }
     },
     "outputs": [],
     "prompt_number": 58
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "### __ create 'all_info' from { mn_vendor,seamless,yelp } [ about 2.5 min ] \n",
      "\n",
      "import sys\n",
      "reload(sys)\n",
      "sys.setdefaultencoding('UTF8')\n",
      "\n",
      "# match_distinct_mnv_phone(tbl='yelp',tbl_id='id')\n",
      "# match_distinct_mnv_phone(tbl='seamless',tbl_id='vend_id')\n",
      "\n",
      "def remove_all_NaN(tbl):\n",
      "    \n",
      "    conn.set_isolation_level(0)\n",
      "    d=pd.read_sql(\"\"\" select column_name cols\n",
      "                      from INFORMATION_SCHEMA.COLUMNS \n",
      "                      where table_name = '%s'\"\"\"%tbl,engine).cols.tolist()\n",
      "\n",
      "    for it in d:\n",
      "        T = {'tbl':tbl,'col':it}\n",
      "        cmd=\"\"\" update %(tbl)s set %(col)s = null\n",
      "                where (\n",
      "                    %(col)s::text='NaN' \n",
      "                    or %(col)s::text='None'\n",
      "                    or %(col)s::text ilike 'nan'\n",
      "                    or %(col)s::text ilike 'nat') \"\"\"%T\n",
      "\n",
      "        conn.set_isolation_level(0)\n",
      "        cur.execute(cmd)\n",
      "        if (it.find('phone')!=-1 and it.find('display')==-1):\n",
      "            cmd=\"\"\" update %(tbl)s set %(col)s = null\n",
      "            where %(col)s = 0 \"\"\"%T\n",
      "            conn.set_isolation_level(0)\n",
      "            cur.execute(cmd)\n",
      "    return\n",
      "\n",
      "def rename_all_cols_with_label(tbl,prefix='',exceptions=[]):\n",
      "    conn.set_isolation_level(0)\n",
      "    d=pd.read_sql(\"\"\" select column_name cols\n",
      "                      from INFORMATION_SCHEMA.COLUMNS \n",
      "                      where table_name = '%s'\"\"\"%tbl,engine).cols.tolist()\n",
      "    base_cmd,s='ALTER TABLE %s '%tbl,''\n",
      "    for it in d:\n",
      "        if exceptions.count(it)==0:\n",
      "            s+=base_cmd+'rename column %(col)s to %(pre)s%(col)s;\\n'%{'col':it,'pre':prefix}\n",
      "    s = s.rstrip(' ,')+';'\n",
      "    conn.set_isolation_level(0)\n",
      "    cur.execute(s)\n",
      "    return\n",
      "\n",
      "def delete_pts_outside_mn():\n",
      "    conn.set_isolation_level(0)\n",
      "    cur.execute(\"\"\"\n",
      "        delete from all_info v\n",
      "        using\n",
      "            (\n",
      "                SELECT ST_Buffer(  ST_ConcaveHull(  (ST_Collect(f.the_geom)), 100, false  ),.0005) mn \n",
      "                FROM ( \n",
      "                SELECT *, (ST_Dump(geom)).geom As the_geom \n",
      "                FROM pluto\n",
      "                ) As f\n",
      "            ) as f2\n",
      "        where not st_containsproperly(mn,v.geom);\n",
      "    \"\"\")\n",
      "    return\n",
      "\n",
      "## ----------------------\n",
      "\n",
      "conn.set_isolation_level(0)\n",
      "cur.execute(\"drop table if exists all_info\")\n",
      "\n",
      "# \n",
      "# CAREFUL with the 'c1' & 'c2' below... the sequence order [ mn_vendors,seamless,yelp ] is essential #\n",
      "# \n",
      "\n",
      "c1 = \"\"\"\n",
      "    select *\n",
      "    from \n",
      "        (select * from mnv mn) as f1\n",
      "    left join seamless sl on f1.seamless_id=sl.vend_id\n",
      "    right join yelp y on f1.yelp_id=y.id\n",
      "    \"\"\"\n",
      "grp = pd.read_sql(c1,engine)\n",
      "grp_cols = grp.columns.tolist()\n",
      "\n",
      "# ---- horizontally split up results with column cnt and rename\n",
      "c2 = \"\"\"   \n",
      "    select mn_cnt,y_cnt,sl_cnt\n",
      "    from\n",
      "        (select count(*) mn_cnt from INFORMATION_SCHEMA.COLUMNS where table_name = 'mnv') as f1,\n",
      "        (select count(*) sl_cnt from INFORMATION_SCHEMA.COLUMNS where table_name = 'seamless') as f2,\n",
      "        (select count(*) y_cnt from INFORMATION_SCHEMA.COLUMNS where table_name = 'yelp') as f3\n",
      "    \"\"\"\n",
      "t=pd.read_sql(c2,engine)\n",
      "t_cols=t.columns.tolist()\n",
      "\n",
      "cnt_dict=dict(zip(t.columns.tolist(),t.ix[0,:].tolist()))\n",
      "tbl_dict={'mn':'mnv','sl':'seamless','y':'yelp'}\n",
      "\n",
      "new_grp_cols,pt=[],0\n",
      "for cnt in sorted(cnt_dict.keys()):\n",
      "    tbl_key = cnt[:cnt.find('_cnt')+1]\n",
      "    next_pt = pt+cnt_dict[cnt]\n",
      "    for i in range(pt,next_pt):\n",
      "        new_grp_cols.append( tbl_key + str(grp_cols[i]) )\n",
      "    pt = next_pt\n",
      "\n",
      "len(new_grp_cols) == sum(cnt_dict.values())\n",
      "grp.columns = [new_grp_cols]\n",
      "\n",
      "# ---- clean up\n",
      "take_cols = ['mn_camis','sl_vend_id','y_gid','mn_vend_name','sl_vend_name','y_vend_name',\n",
      "             'mn_address','sl_address','y_display_address','y_neighborhoods','mn_zipcode',\n",
      "             'sl_zipcode','y_postal_code','mn_phone','sl_phone','y_display_phone',\n",
      "             'sl_sl_link','y_url','y_website','y_menu_page','y_menu_date_updated','y_menu_provider',\n",
      "             'sl_inactive','y_is_claimed','y_is_closed','y_online_ordering','y_extra_info','y_hours',\n",
      "             'mn_cuisinecode','sl_cuisine','y_categories','sl_description',\n",
      "             'sl_price','y_price_range','sl_deliv_min','sl_deliv_fee','sl_perc_fee','sl_pickup_est','sl_deliv_est',\n",
      "             'sl_rating','sl_rating_total','sl_rating_perc','y_rating','y_review_count','sl_reviews',\n",
      "             'mn_inspdate','sl_upd_search_links','sl_upd_vend_content','y_last_api_update','y_hours_updated']\n",
      "grp = grp.ix[:,take_cols]\n",
      "take_cols_with_geom = take_cols + ['geom']\n",
      "\n",
      "re_encode_cols = ['mn_vend_name','sl_vend_name','y_vend_name',\n",
      "                  'sl_description']\n",
      "for it in re_encode_cols:\n",
      "    grp[it] = grp[it].map(lambda s: s if type(s)==NoneType else codecs.encode(s,'utf8','ignore'))\n",
      "\n",
      "phone_cols = ['mn_phone','sl_phone']\n",
      "for it in phone_cols:\n",
      "    grp[it] = grp[it].map(lambda s: 0 if s.is_integer()==False else int(s))\n",
      "\n",
      "title_cols = ['mn_vend_name','sl_vend_name','y_vend_name',\n",
      "              'mn_address','sl_address','y_display_address']\n",
      "for it in title_cols:\n",
      "    grp[it] = grp[it].map(lambda s: str(s).title())\n",
      "\n",
      "datetime_cols = ['mn_inspdate','sl_upd_search_links','sl_upd_vend_content','y_last_api_update',\n",
      "                 'y_menu_date_updated','y_hours_updated']\n",
      "for it in datetime_cols:\n",
      "    grp[it] = grp[it].map(str)\n",
      "\n",
      "# ---- push to pgSQL\n",
      "grp['y_hours'] = grp.y_hours.map(lambda s: None if (str(s)=='nan' or str(s)=='None') else '\\n'.join([it[0]+': '+it[1] for it in eval(str(s))]) )\n",
      "grp['y_extra_info'] = grp.y_extra_info.map(lambda s: None if (str(s)=='nan' or str(s)=='None') else '\\n'.join([it[0]+': '+it[1] for it in eval(str(s))]) )\n",
      "conn.set_isolation_level(0)\n",
      "cur.execute( \"\"\"drop table if exists all_info;\"\"\")\n",
      "grp.to_sql('all_info',engine)\n",
      "conn.set_isolation_level(0)\n",
      "cur.execute(\"\"\"\n",
      "    alter table all_info add column glob_id serial primary key;\n",
      "    update all_info set glob_id = nextval(pg_get_serial_sequence('all_info','glob_id'));\n",
      "    \"\"\")\n",
      "\n",
      "# ---- make first part of DB\n",
      "conn.set_isolation_level(0)\n",
      "cur.execute(\"\"\"\n",
      "    ALTER TABLE all_info \n",
      "        drop column index;\n",
      "\n",
      "    ALTER TABLE all_info \n",
      "        add column lat double precision,\n",
      "        add column lon double precision,\n",
      "        add column geom geometry(Point,4326),\n",
      "        add column new_notes text,\n",
      "        add column follow_up text,\n",
      "        add column old_notes text,\n",
      "        add column mark_for_edit text;\n",
      "\n",
      "    UPDATE all_info c set geom = t_geom\n",
      "    from (select t.camis t_id,t.geom t_geom from mnv t where geom is not null) as f1\n",
      "    where c.mn_camis = t_id\n",
      "    and (c.mn_camis is not null and ( not c.mn_camis::text ilike 'nan' ) );\n",
      "\n",
      "    UPDATE all_info c set geom = t_geom\n",
      "    from (select t.gid t_id,t.geom t_geom from yelp t where geom is not null) as f1\n",
      "    where c.y_gid = t_id\n",
      "    and (c.y_gid is not null and ( not c.y_gid::text ilike 'nan' ) );\n",
      "\n",
      "    UPDATE all_info c set geom = t_geom\n",
      "    from (select t.vend_id t_id,t.geom t_geom from seamless t where geom is not null) as f1\n",
      "    where c.sl_vend_id = t_id\n",
      "    and (c.sl_vend_id is not null and ( not c.sl_vend_id::text ilike 'nan' ) );\n",
      "    \"\"\")\n",
      "\n",
      "# ---- account for all mnv records\n",
      "uniq_grp_mn_ids = grp[grp.mn_camis.isnull()==False].mn_camis.unique().tolist()\n",
      "all_mn_ids = pd.read_sql(\"\"\"\n",
      "    select distinct camis from mnv \n",
      "    where geom is not null\n",
      "    and (\n",
      "        camis is not null \n",
      "        and ( not camis::text ilike 'nan' )\n",
      "        and ( not camis::text ilike 'nat' ) \n",
      "        )\n",
      "    \"\"\",engine)\n",
      "if not sorted(all_mn_ids.camis.unique().tolist())==sorted(uniq_grp_mn_ids):\n",
      "    \n",
      "    rem_mn_ids = all_mn_ids[(all_mn_ids.camis.isin(uniq_grp_mn_ids)==False)].camis.unique().tolist()\n",
      "    conn.set_isolation_level(0)\n",
      "    cur.execute( \"\"\"drop table if exists tmp;\"\"\")\n",
      "    pd.DataFrame({'camis':rem_mn_ids}).to_sql('tmp',engine)\n",
      "    conn.set_isolation_level(0)\n",
      "    cur.execute(\"\"\"\n",
      "        alter table tmp add column gid serial primary key;\n",
      "        update tmp set gid = nextval(pg_get_serial_sequence('tmp','gid'));\n",
      "    \"\"\")\n",
      "\n",
      "    conn.set_isolation_level(0)\n",
      "    cur.execute( \"\"\"drop table if exists tmp2;\"\"\")\n",
      "    conn.set_isolation_level(0)\n",
      "    cur.execute(\"\"\"\n",
      "        create table tmp2 as\n",
      "        select m.* from \n",
      "            mnv m,\n",
      "            (select array_agg(t.camis) A from tmp t where t.camis is not null) as f1\n",
      "        where m.geom is not null\n",
      "        and (array[m.camis] && A);\n",
      "        \n",
      "        alter table tmp2 add column gid serial primary key;\n",
      "        update tmp2 set gid = nextval(pg_get_serial_sequence('tmp2','gid'));\n",
      "        \"\"\",engine)\n",
      "\n",
      "    rename_all_cols_with_label(tbl='tmp2',prefix='mn_',exceptions=['geom'])\n",
      "    \n",
      "    rem_mn_records = pd.read_sql('select * from tmp2',engine)\n",
      "    new_rem_records_cols = rem_mn_records.columns.map(str).tolist()\n",
      "\n",
      "    if not len(all_mn_ids) == len(uniq_grp_mn_ids) + len(rem_mn_records.mn_camis.unique().tolist()):\n",
      "        print 'Missing mnv records'\n",
      "        raise SystemError()\n",
      "\n",
      "    # -- Remove all 'NaN'\n",
      "    tbl = 'tmp2'\n",
      "    remove_all_NaN(tbl)\n",
      "\n",
      "    insert_cols = [it for it in new_rem_records_cols if take_cols_with_geom.count(it)>0]\n",
      "    insert_cols = str(insert_cols).strip('[]').replace(\"'\",'')\n",
      "    T = {'insert_cols':insert_cols}\n",
      "    cmd = \"\"\"   insert into all_info (%(insert_cols)s)\n",
      "                select %(insert_cols)s from tmp2\n",
      "          \"\"\"%T\n",
      "    conn.set_isolation_level(0)\n",
      "    cur.execute(cmd)\n",
      "\n",
      "# ---- account for all yelp records\n",
      "uniq_grp_y_gids = grp[(grp.y_gid.map(lambda s: True if str(s)[0].isdigit() else False)==True)].y_gid.unique().tolist()\n",
      "all_y_gids = pd.read_sql(\"\"\"\n",
      "    select distinct gid from yelp \n",
      "    where geom is not null\n",
      "    and (\n",
      "        gid is not null \n",
      "        and ( not gid::text ilike 'nan' )\n",
      "        and ( not gid::text ilike 'nat' ) \n",
      "        )\n",
      "    \"\"\",engine)\n",
      "if not sorted(all_y_gids.gid.unique().tolist())==sorted(uniq_grp_y_gids):\n",
      "    \n",
      "    rem_y_gids = all_y_gids[(all_y_gids.gid.isin(uniq_grp_y_gids)==False)].gid.unique().tolist()\n",
      "    conn.set_isolation_level(0)\n",
      "    cur.execute( \"\"\"drop table if exists tmp;\"\"\")\n",
      "    pd.DataFrame({'gid':rem_y_gids}).to_sql('tmp',engine)\n",
      "    conn.set_isolation_level(0)\n",
      "    cur.execute(\"\"\"\n",
      "        alter table tmp add column tid serial primary key;\n",
      "        update tmp set tid = nextval(pg_get_serial_sequence('tmp','tid'));\n",
      "        \"\"\")\n",
      "    \n",
      "    conn.set_isolation_level(0)\n",
      "    cur.execute(\"drop table if exists tmp2;\")\n",
      "    conn.set_isolation_level(0)\n",
      "    cur.execute(\"\"\"\n",
      "        create table tmp2 as\n",
      "        select y.* from \n",
      "            yelp y,\n",
      "            (select array_agg(t.gid::integer) A from tmp t where t.gid is not null) as f1\n",
      "        where y.geom is not null\n",
      "        and (array[y.gid] && A);\n",
      "        \n",
      "        alter table tmp2 add column tid serial primary key;\n",
      "        update tmp2 set tid = nextval(pg_get_serial_sequence('tmp2','tid'));\n",
      "        \"\"\",engine)\n",
      "    \n",
      "    rename_all_cols_with_label(tbl='tmp2',prefix='y_',exceptions=['geom'])\n",
      "    \n",
      "    rem_y_records = pd.read_sql('select * from tmp2',engine)\n",
      "    new_rem_records_cols = rem_y_records.columns.map(str).tolist()\n",
      "\n",
      "    if not len(all_y_gids) == len(uniq_grp_y_gids) + len(rem_y_records.gid.unique().tolist()):\n",
      "        print 'Missing yelp records'\n",
      "        raise SystemError()\n",
      "    \n",
      "    tbl='tmp2'\n",
      "    remove_all_NaN(tbl)\n",
      "    \n",
      "    new_rem_records_cols = ['y_'+str(it) for it in rem_y_records.columns.tolist()]\n",
      "    rem_y_records.columns = [new_rem_records_cols]\n",
      "    z = rem_y_records.ix[:,take_cols_with_geom]\n",
      "    conn.set_isolation_level(0)\n",
      "    cur.execute('drop table if exists tmp;')\n",
      "    z.to_sql('tmp',engine)\n",
      "\n",
      "    # -- Remove all 'NaN'\n",
      "    tbl = 'tmp'\n",
      "    remove_all_NaN(tbl)\n",
      "    \n",
      "    insert_cols = [it for it in new_rem_records_cols if take_cols_with_geom.count(it)>0]\n",
      "    insert_cols = str(insert_cols).strip('[]').replace(\"'\",'')\n",
      "    T = {'insert_cols':insert_cols}\n",
      "    cmd = \"\"\"   insert into all_info (%(insert_cols)s)\n",
      "                select %(insert_cols)s from tmp\n",
      "          \"\"\"%T\n",
      "    conn.set_isolation_level(0)\n",
      "    cur.execute(cmd)\n",
      "\n",
      "# ---- account for all seamless records\n",
      "uniq_grp_sl_ids = grp[grp.sl_vend_id.map(lambda s: s.is_integer())==True].sl_vend_id.unique().tolist()\n",
      "all_sl_ids = pd.read_sql(\"\"\"\n",
      "    select distinct vend_id id from seamless \n",
      "    where geom is not null\n",
      "    and (\n",
      "        vend_id is not null \n",
      "        and ( not vend_id::text ilike 'nan' )\n",
      "        and ( not vend_id::text ilike 'nat' ) \n",
      "        )\n",
      "    \"\"\",engine)\n",
      "if not sorted(all_sl_ids.id.unique().tolist())==sorted(uniq_grp_sl_ids):\n",
      "    \n",
      "    rem_sl_ids = all_sl_ids[(all_sl_ids.id.isin(uniq_grp_sl_ids)==False)].id.unique().tolist()\n",
      "    conn.set_isolation_level(0)\n",
      "    cur.execute( \"\"\"drop table if exists tmp;\"\"\")\n",
      "    pd.DataFrame({'vend_id':rem_sl_ids}).to_sql('tmp',engine)\n",
      "    conn.set_isolation_level(0)\n",
      "    cur.execute(\"\"\"\n",
      "        alter table tmp add column tid serial primary key;\n",
      "        update tmp set tid = nextval(pg_get_serial_sequence('tmp','tid'));\n",
      "        \"\"\")\n",
      "    \n",
      "    conn.set_isolation_level(0)\n",
      "    cur.execute( \"\"\"drop table if exists tmp2;\"\"\")\n",
      "    conn.set_isolation_level(0)\n",
      "    cur.execute(\"\"\"\n",
      "        create table tmp2 as\n",
      "        select s.* from \n",
      "            seamless s,\n",
      "            (select array_agg(t.vend_id) A from tmp t) as f1\n",
      "        where s.geom is not null\n",
      "        and (array[s.vend_id] && A);\n",
      "    \n",
      "        alter table tmp2 add column tid serial primary key;\n",
      "        update tmp2 set tid = nextval(pg_get_serial_sequence('tmp2','tid'));\n",
      "        \"\"\")\n",
      "    \n",
      "    rename_all_cols_with_label(tbl='tmp2',prefix='sl_',exceptions=['geom'])\n",
      "\n",
      "    rem_sl_records = pd.read_sql('select * from tmp2',engine)\n",
      "    new_rem_records_cols = rem_sl_records.columns.map(str).tolist()\n",
      "\n",
      "    if not len(all_sl_ids) == len(uniq_grp_sl_ids) + len(rem_sl_records.sl_vend_id.unique().tolist()):\n",
      "        print 'Missing seamless records'\n",
      "        raise SystemError()\n",
      "\n",
      "    # -- Remove all 'NaN'\n",
      "    tbl = 'tmp'\n",
      "    remove_all_NaN(tbl)\n",
      "    \n",
      "    insert_cols = [it for it in new_rem_records_cols if take_cols_with_geom.count(it)>0]\n",
      "    insert_cols = str(insert_cols).strip('[]').replace(\"'\",'')\n",
      "    T = {'insert_cols':insert_cols}\n",
      "    cmd = \"\"\"   insert into all_info (%(insert_cols)s)\n",
      "                select %(insert_cols)s from tmp2\n",
      "          \"\"\"%T\n",
      "    conn.set_isolation_level(0)\n",
      "    cur.execute(cmd)\n",
      "\n",
      "re_encode_cols = ['mn_vend_name','sl_vend_name','y_vend_name',\n",
      "                  'sl_description']\n",
      "for it in re_encode_cols:\n",
      "    grp[it] = grp[it].map(lambda s: None if (type(s)==NoneType or str(s).lower()=='nan') else codecs.encode(s,'utf8','ignore'))\n",
      "\n",
      "title_cols = ['mn_vend_name','sl_vend_name','y_vend_name']\n",
      "for it in title_cols:\n",
      "    grp[it] = grp[it].map(lambda s: str(s).title())\n",
      "\n",
      "datetime_cols = ['mn_inspdate','sl_upd_search_links','sl_upd_vend_content','y_last_api_update',\n",
      "                 'y_menu_date_updated','y_hours_updated']\n",
      "for it in datetime_cols:\n",
      "    grp[it] = grp[it].map(str)\n",
      "    \n",
      "# update geom to match Google Coord/Projection System\n",
      "\n",
      "\n",
      "# conn.set_isolation_level(0)\n",
      "# cur.execute(\"\"\"\n",
      "# ALTER TABLE all_info \n",
      "#     ALTER COLUMN geom TYPE geometry(Point, 3857) USING ST_Transform(ST_SetSRID(geom,4326),3857);\n",
      "\n",
      "# UPDATE all_info set\n",
      "#     lat = st_y(geom::geometry(Point,3857)),\n",
      "#     lon = st_x(geom::geometry(Point,3857));\n",
      "\n",
      "# \"\"\")\n",
      "\n",
      "conn.set_isolation_level(0)\n",
      "cur.execute(\"\"\"\n",
      "    UPDATE all_info set\n",
      "        lat = st_y(ST_SetSRID(geom::geometry(Point,4326),4326)),\n",
      "        lon = st_x(ST_SetSRID(geom::geometry(Point,4326),4326));\n",
      "\n",
      "    \"\"\")\n",
      "\n",
      "\n",
      "# -- Final Yelp check\n",
      "all_y_id = pd.read_sql(\"\"\"\n",
      "    select distinct gid id from yelp\n",
      "    where (gid is not null and gid::text!='NaN' and geom is not null)\n",
      "    \"\"\",engine)\n",
      "y_id_in_grp = pd.read_sql(\"\"\"\n",
      "    select distinct y_gid::integer id from all_info \n",
      "    where (y_gid is not null and y_gid::text!='NaN');\n",
      "    \"\"\",engine)\n",
      "if not sorted(all_y_id)==sorted(y_id_in_grp):\n",
      "    print 'missing yelp records_2'\n",
      "\n",
      "# -- Final Seamless check\n",
      "all_sl_id = pd.read_sql(\"\"\"\n",
      "    select distinct vend_id id from seamless\n",
      "    where (vend_id is not null and geom is not null)\n",
      "    \"\"\",engine)\n",
      "sl_id_in_grp = pd.read_sql(\"\"\"\n",
      "    select distinct sl_vend_id id from all_info \n",
      "    where (sl_vend_id is not null);\n",
      "    \"\"\",engine)\n",
      "if not sorted(all_sl_id)==sorted(sl_id_in_grp):\n",
      "    print 'missing seamless records_2'\n",
      "\n",
      "remove_all_NaN(tbl='all_info')\n",
      "\n",
      "conn.set_isolation_level(0)\n",
      "cur.execute(\"\"\"\n",
      "    ALTER TABLE all_info \n",
      "        ALTER COLUMN mn_zipcode TYPE bigint USING mn_zipcode::bigint,\n",
      "        ALTER COLUMN sl_zipcode TYPE bigint USING sl_zipcode::bigint,\n",
      "        ALTER COLUMN y_postal_code TYPE bigint USING y_postal_code::bigint,\n",
      "        ALTER COLUMN mn_camis TYPE bigint USING mn_camis::bigint,\n",
      "        ALTER COLUMN y_gid TYPE bigint USING y_gid::bigint,\n",
      "        ALTER COLUMN mn_cuisinecode TYPE bigint USING mn_cuisinecode::bigint,\n",
      "        ALTER COLUMN sl_rating TYPE bigint USING sl_rating::bigint,\n",
      "        ALTER COLUMN sl_rating_total TYPE bigint USING sl_rating_total::bigint,\n",
      "        ALTER COLUMN y_rating TYPE bigint USING y_rating::bigint,\n",
      "        ALTER COLUMN y_review_count TYPE bigint USING y_review_count::bigint,\n",
      "        ALTER COLUMN sl_reviews TYPE bigint USING sl_reviews::bigint;\n",
      "\n",
      "    ALTER TABLE all_info \n",
      "        ALTER COLUMN mn_inspdate TYPE timestamp without time zone USING mn_inspdate::timestamp without time zone,\n",
      "        ALTER COLUMN y_menu_date_updated TYPE timestamp without time zone USING y_menu_date_updated::timestamp without time zone,\n",
      "        ALTER COLUMN sl_upd_search_links TYPE timestamp with time zone USING sl_upd_search_links::timestamp with time zone,\n",
      "        ALTER COLUMN sl_upd_vend_content TYPE timestamp with time zone USING sl_upd_vend_content::timestamp with time zone,\n",
      "        ALTER COLUMN y_last_api_update TYPE timestamp with time zone USING y_last_api_update::timestamp with time zone,\n",
      "        ALTER COLUMN y_hours_updated TYPE timestamp with time zone USING y_hours_updated::timestamp with time zone;\n",
      "\n",
      "    ALTER TABLE all_info\n",
      "        ADD COLUMN map_group integer;\n",
      "    \"\"\")\n",
      "\n",
      "delete_pts_outside_mn()"
     ],
     "language": "python",
     "metadata": {
      "code_folding": [
       0
      ],
      "run_control": {
       "read_only": false
      }
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "### All_vend.csv to gDrive\n",
      "T = {'base_file_path':'/Users/admin/gDrive/Aporo/marketing/all_info',\n",
      "     'file_ext':'.csv'}\n",
      "map_groups = pd.read_sql('select distinct map_group from all_info',engine).map_group.tolist()\n",
      "drop_cols = ['geom','sl_upd_search_links','mn_cuisinecode',\n",
      "                 'mn_zipcode','sl_zipcode','y_postal_code',\n",
      "                 'y_last_api_update','sl_rating_perc','map_group']\n",
      "for m_grp in map_groups:\n",
      "    T.update({'m_grp':m_grp})\n",
      "    grp_cnt = pd.read_sql('select count(*) c from all_info where map_group = %(m_grp)s'%T,engine).c[0]\n",
      "    if grp_cnt<=2000:\n",
      "        grp = pd.read_sql(\"\"\"\n",
      "                select * from all_info\n",
      "                where map_group = %(m_grp)s \n",
      "                order by glob_id asc\"\"\"%T,engine)\n",
      "        grp = grp.drop(drop_cols,axis=1)\n",
      "        f_path = T['base_file_path'] + '_grp_%(m_grp)s'%T + T['file_ext']\n",
      "        grp.to_csv(f_path,index=False,encoding='utf-8')\n",
      "    else:\n",
      "        grp_size = 2000\n",
      "        grp_num = int(2 + float(grp_cnt)//float(grp_size))\n",
      "        offset = 0\n",
      "        z=[]\n",
      "        T.update({'limit':grp_size,\n",
      "                  'offset':offset})\n",
      "        for i in range(grp_num):\n",
      "            grp = pd.read_sql(\"\"\"\n",
      "                select * from all_info \n",
      "                where map_group = %(m_grp)s\n",
      "                order by glob_id desc \n",
      "                limit %(limit)s offset %(offset)s\n",
      "                              \"\"\"%T,engine)\n",
      "            z.append(len(grp))\n",
      "            T.update({'offset':i*grp_size})\n",
      "            grp = grp.drop(drop_cols,axis=1)\n",
      "            f_path = T['base_file_path'] + '_grp_%(m_grp)s_'%T + str(i) + T['file_ext']\n",
      "            if os_path.isfile(f_path):\n",
      "                os_cmd('rm '+f_path)\n",
      "            grp.to_csv(f_path,index=False,encoding='utf-8')\n",
      "        \n",
      "    "
     ],
     "language": "python",
     "metadata": {
      "code_folding": [
       0
      ],
      "run_control": {
       "read_only": false
      }
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "# Divide Data by Geolocation\n",
      "\n",
      "import geopandas as gd\n",
      "%matplotlib inline\n",
      "\n",
      "def perpendicular_degrees_from_degrees(st_deg,sorted_clockwise='always'):\n",
      "    pos = st_deg + 90\n",
      "    if pos>360: pos = pos - 360\n",
      "    neg = st_deg - 90\n",
      "    if neg<0:   neg = 360 + neg\n",
      "    a,b = sorted([pos,neg])\n",
      "    return a,b\n",
      "\n",
      "show_info=True\n",
      "min_v,max_v = 1900.0,2000.0\n",
      "street_to_divide_by = ''\n",
      "T={}\n",
      "\n",
      "# divide by points\n",
      "bot_pt = (40.699857,-74.025184)\n",
      "top_pt = (40.88103,-73.89526)\n",
      "\n",
      "# divide-by-street       NOT FULLY SETUP YET (2014.11.22)\n",
      "# street_to_divide_by = 42\n",
      "\n",
      "# ---------------------\n",
      "# ---------------------\n",
      "\n",
      "\n",
      "# set all map_group to null\n",
      "conn.set_isolation_level(0)\n",
      "cur.execute(\"\"\"\n",
      "    update all_info v\n",
      "    set map_group = null\n",
      "    \"\"\")\n",
      "\n",
      "# [ divide-by-street ] get angles\n",
      "if street_to_divide_by != '':\n",
      "    c=  \"\"\"\n",
      "        SELECT \n",
      "            (ST_Y(s_pt), ST_X(s_pt)) s_pts,\n",
      "            (ST_Y(e_pt), ST_X(e_pt)) e_pts,\n",
      "            degrees( ST_Azimuth(s_pt,e_pt) ) degrees,\n",
      "            g\n",
      "        FROM\n",
      "            (\n",
      "            SELECT \n",
      "                ST_StartPoint(geom) s_pt,\n",
      "                ST_EndPoint(geom) e_pt,\n",
      "                geom g\n",
      "            FROM \n",
      "                address_idx \n",
      "            WHERE geom is not null and street ilike '%s%s'\n",
      "                ) as f1\n",
      "        \"\"\"%(street_to_divide_by,'%%')\n",
      "\n",
      "    g = gd.read_postgis(c,conn,'g')\n",
      "\n",
      "    st_start_coords = dict(zip(['lon','lat'],g.s_pts[0].strip('()').split(',')))\n",
      "    st_end_coords = dict(zip(['lon','lat'],g.e_pts[0].strip('()').split(',')))\n",
      "    T.update({'st_start_coords':\"SELECT ST_GeomFromText('POINT(%(lat)s %(lon)s)',4326) t_l_pt\"%st_start_coords,\n",
      "              'st_end_coords':\"SELECT ST_GeomFromText('POINT(%(lat)s %(lon)s)',4326) t_r_pt\"%st_end_coords})\n",
      "\n",
      "    st_angle = g.degrees[0]\n",
      "    u_angle,d_angle = perpendicular_degrees_from_degrees(st_angle)\n",
      "    r_angle,l_angle = perpendicular_degrees_from_degrees(u_angle)\n",
      "    T.update({'l_angle':l_angle,'r_angle':r_angle,\n",
      "              'd_angle':d_angle,'u_angle':u_angle})\n",
      "\n",
      "    # Using angle, find approximate lines on each side where line doesn't touch MN\n",
      "\n",
      "# make Manhattan geoms and start geom collection\n",
      "base_geom = []\n",
      "MN =\"\"\"\n",
      "    SELECT \n",
      "        ST_AsEWKT( mn_geom ) mn_geom,\n",
      "        ST_AsEWKT( mn_e ) mn_e\n",
      "    FROM (\n",
      "        SELECT mn_geom,ST_Envelope(mn_geom) mn_e\n",
      "        FROM (\n",
      "            SELECT ST_Buffer(  ST_ConcaveHull(  (ST_Collect(f.the_geom)), 100, false  ), \n",
      "                               .0005) as mn_geom \n",
      "            FROM ( SELECT *, (ST_Dump(geom)).geom As the_geom \n",
      "            FROM pluto) As f\n",
      "            ) as f1\n",
      "        ) as f2;\n",
      "    \"\"\"\n",
      "p = pd.read_sql(MN,conn)\n",
      "\n",
      "mn_geom=\"ST_GeomFromText('%s',4326) geom\"%p.mn_geom[0].split(';')[1]\n",
      "base_geom.extend(gd.read_postgis('select %s'%mn_geom,conn).geom)\n",
      "if street_to_divide_by != '':\n",
      "    mn_e=\"ST_GeomFromText('%s',4326) geom\"%p.mn_e[0].split(';')[1]\n",
      "    base_geom.extend(gd.read_postgis('select %s'%mn_e,conn).geom)\n",
      "\n",
      "    outer_box = pd.read_sql('select box2d(%s) b;'%mn_e_b.replace('geom',''),conn)\n",
      "    a_b_l,a_t_r = outer_box.b[0].strip('BOX()').split(',') # absolute bottom left, etc...\n",
      "    a_b_l,a_t_r = a_b_l.split(' '),a_t_r.split(' ')\n",
      "\n",
      "\n",
      "# make points (bot_pt,top_pt) at each end of MN\n",
      "bot_pt_pg = \"SELECT ST_GeomFromText('POINT(%s %s)',4326) bot_pt\"%(str(bot_pt[1]),str(bot_pt[0]))\n",
      "top_pt_pg = \"SELECT ST_GeomFromText('POINT(%s %s)',4326) top_pt\"%(str(top_pt[1]),str(top_pt[0]))\n",
      "base_geom.extend(gd.read_postgis(bot_pt_pg,conn,'bot_pt').bot_pt)\n",
      "base_geom.extend(gd.read_postgis(top_pt_pg,conn,'top_pt').top_pt)\n",
      "\n",
      "\n",
      "# make line (k_line) connecting points\n",
      "T = {'bot_pt':bot_pt_pg,'top_pt':top_pt_pg}\n",
      "c=\"\"\"\n",
      "    select ST_MakeLine(bot_pt,top_pt) k_line\n",
      "    from\n",
      "        (%(bot_pt)s) as f1,\n",
      "        (%(top_pt)s) as f2\n",
      "    \"\"\"%T\n",
      "base_geom.extend(gd.read_postgis(c,conn,'k_line').k_line)\n",
      "\n",
      "\n",
      "# get length (k_line_len) of k_line (google says 12.9 miles)\n",
      "feet_per_meter = 3.28084\n",
      "miles_per_foot = 0.000189394\n",
      "miles_per_meter = 0.000621371\n",
      "T.update({'m_to_ft':feet_per_meter,'ft_to_mi':miles_per_foot,'m_to_mi':miles_per_meter})\n",
      "\n",
      "c=\"\"\"\n",
      "    select ( %(m_to_mi)s * ST_Distance_Sphere(bot_pt,top_pt) ) distance_in_miles\n",
      "    from\n",
      "        (%(bot_pt)s) as f1,\n",
      "        (%(top_pt)s) as f2\n",
      "    \"\"\"%T\n",
      "k_line_len = pd.read_sql(c,engine).distance_in_miles[0]\n",
      "\n",
      "proj_lat_dist = (k_line_len / 5.0) * ( 1 / T['m_to_mi'] )\n",
      "proj_lon_dist = (k_line_len / 10.0) * ( 1 / T['m_to_mi'] )\n",
      "T.update({'proj_lat_dist':proj_lat_dist,\n",
      "          'proj_lon_dist':proj_lon_dist})\n",
      "\n",
      "\n",
      "# get azimuth angle (k_line_angle) of connecting line\n",
      "c=\"\"\"\n",
      "    select degrees( ST_Azimuth(bot_pt,top_pt) ) degrees\n",
      "    from\n",
      "        (%(bot_pt)s) as f1,\n",
      "        (%(top_pt)s) as f2\n",
      "    \"\"\"%T\n",
      "k_line_angle = pd.read_sql(c,engine).degrees[0]\n",
      "\n",
      "\n",
      "# get angles (l_angle,r_angle) for perpendicular lines on left and right\n",
      "r_angle,l_angle = perpendicular_degrees_from_degrees(k_line_angle)\n",
      "u_angle,d_angle = perpendicular_degrees_from_degrees(l_angle)\n",
      "T.update({'l_angle':l_angle,'r_angle':r_angle,\n",
      "          'd_angle':d_angle,'u_angle':u_angle})\n",
      "\n",
      "\n",
      "# get base points by projecting perpendicular at bottom of k_line\n",
      "c=\"\"\"\n",
      "    select \n",
      "        ST_MakeLine(l_pt,r_pt) new_line,\n",
      "        (ST_Y(l_pt), ST_X(l_pt)) l_pt,\n",
      "        (ST_Y(r_pt), ST_X(r_pt)) r_pt\n",
      "    from\n",
      "        (\n",
      "        select \n",
      "            ST_Project(bot_pt,%(proj_lat_dist)s,radians(%(l_angle)s))::geometry(Point,4326) l_pt,\n",
      "            ST_Project(bot_pt,%(proj_lat_dist)s,radians(%(r_angle)s))::geometry(Point,4326) r_pt\n",
      "        from\n",
      "            (%(bot_pt)s) as f1\n",
      "        ) as f3\n",
      "    \"\"\"%T\n",
      "g = gd.read_postgis(c,conn,'new_line')\n",
      "l_coords = dict(zip(['lon','lat'],g.l_pt[0].strip('()').split(',')))\n",
      "r_coords = dict(zip(['lon','lat'],g.r_pt[0].strip('()').split(',')))\n",
      "T.update({'b_l_pt':\"SELECT ST_GeomFromText('POINT(%(lat)s %(lon)s)',4326) b_l_pt\"%l_coords,\n",
      "          'b_r_pt':\"SELECT ST_GeomFromText('POINT(%(lat)s %(lon)s)',4326) b_r_pt\"%r_coords})\n",
      "def get_point_cnt(T,geoms=[]):\n",
      "    geoms=[]\n",
      "\n",
      "    ## print '\\n\\n THIS IS NOT CHECKING FOR POINTS BELOW STARTING POINT. \\n\\n'\n",
      "\n",
      "    # project points perpendicular line (T_line) up k_line\n",
      "    c=\"\"\"\n",
      "        select \n",
      "            ST_MakeLine(t_l_pt,t_r_pt) new_line,\n",
      "            (ST_Y(t_l_pt), ST_X(t_l_pt)) t_l_pt,\n",
      "            (ST_Y(t_r_pt), ST_X(t_r_pt)) t_r_pt\n",
      "        from\n",
      "            (\n",
      "            select \n",
      "                ST_Project(b_l_pt,%(proj_lon_dist)s,radians(%(u_angle)s))::geometry(Point,4326) t_l_pt,\n",
      "                ST_Project(b_r_pt,%(proj_lon_dist)s,radians(%(u_angle)s))::geometry(Point,4326) t_r_pt\n",
      "            from\n",
      "                (%(b_l_pt)s) as f1,\n",
      "                (%(b_r_pt)s) as f2\n",
      "            ) as f3\n",
      "        \"\"\"%T\n",
      "    g = gd.read_postgis(c,conn,'new_line')\n",
      "    l_coords = dict(zip(['lon','lat'],g.t_l_pt[0].strip('()').split(',')))\n",
      "    r_coords = dict(zip(['lon','lat'],g.t_r_pt[0].strip('()').split(',')))\n",
      "    T.update({'t_l_pt':\"SELECT ST_GeomFromText('POINT(%(lat)s %(lon)s)',4326) t_l_pt\"%l_coords,\n",
      "              't_r_pt':\"SELECT ST_GeomFromText('POINT(%(lat)s %(lon)s)',4326) t_r_pt\"%r_coords})\n",
      "\n",
      "    # make box with points from two lines\n",
      "    c=\"\"\"\n",
      "        SELECT \n",
      "            ST_ConcaveHull( ST_LineFromMultiPoint( ST_Collect( array[t_l_pt,t_r_pt,b_l_pt,b_r_pt] ) ),\n",
      "                            100,False) box               \n",
      "        FROM\n",
      "            (%(t_l_pt)s) as f1,\n",
      "            (%(t_r_pt)s) as f2,\n",
      "            (%(b_l_pt)s) as f3,\n",
      "            (%(b_r_pt)s) as f4    \n",
      "        \"\"\"%T\n",
      "    g = gd.read_postgis(c,conn,'box')\n",
      "    geoms.extend(g.box)\n",
      "    \n",
      "    # check if any points in box\n",
      "    c=\"\"\"\n",
      "        SELECT count(*) cnt\n",
      "        FROM \n",
      "            all_info v,\n",
      "            (\n",
      "                SELECT \n",
      "                    ST_ConcaveHull( ST_LineFromMultiPoint( ST_Collect( array[t_l_pt,t_r_pt,b_l_pt,b_r_pt] ) ),\n",
      "                                    100,False) box               \n",
      "                FROM\n",
      "                    (%(t_l_pt)s) as f1,\n",
      "                    (%(t_r_pt)s) as f2,\n",
      "                    (%(b_l_pt)s) as f3,\n",
      "                    (%(b_r_pt)s) as f4\n",
      "            ) as f5\n",
      "        WHERE ST_ContainsProperly(box,v.geom);\n",
      "        \"\"\"%T\n",
      "    cnt = pd.read_sql(c,conn).cnt[0]\n",
      "    \n",
      "    return cnt,T,geoms\n",
      "\n",
      "# final iteration\n",
      "def update_map_groups(min_v=1800.0,max_v=2000.0,\n",
      "                      base_geom=[],show_info=False):\n",
      "    min_v,max_v = float(min_v),float(max_v)\n",
      "    box_num,saved_boxes,last_move = 0,[],'start'\n",
      "    while last_move!='complete':\n",
      "        if show_info==True: print box_num,'\\t\\t box_num'\n",
      "        cnt = 0\n",
      "        incr_seg,last_cnt = 0,0\n",
      "        while not (min_v<=cnt<=max_v):\n",
      "            cnt,T_tmp,box = get_point_cnt(T)\n",
      "            if cnt<min_v:\n",
      "                if (last_move=='increase') and (cnt==last_cnt): \n",
      "                    last_move = 'complete'\n",
      "                    break\n",
      "                else:\n",
      "                    incr_seg = (min_v-cnt)/min_v\n",
      "                    T.update({'proj_lon_dist':T['proj_lon_dist']+(incr_seg*T['proj_lon_dist'])})\n",
      "                    last_move = 'increase'\n",
      "                    last_cnt = cnt\n",
      "\n",
      "            elif cnt>max_v:\n",
      "                if incr_seg==0:  decr_seg = 0.5\n",
      "                else: \n",
      "                    decr_seg = .5 * incr_seg\n",
      "                    incr_seg = decr_seg\n",
      "                T.update({'proj_lon_dist':T['proj_lon_dist']-(decr_seg*T['proj_lon_dist'])})\n",
      "                last_move = 'decrease'\n",
      "            if show_info==True: \n",
      "                if not (min_v<=cnt<=max_v):\n",
      "                    print cnt,'\\t\\t Last Count'\n",
      "                else:\n",
      "                    print cnt,'\\t\\t Save Count'\n",
      "\n",
      "\n",
      "        saved_boxes.extend(box)\n",
      "\n",
      "        box_txt = \"ST_GeomFromText('%s',4326)\"%box[0].to_wkt()\n",
      "        conn.set_isolation_level(0)\n",
      "        cur.execute(\"\"\"\n",
      "            update all_info v\n",
      "            set map_group = %s\n",
      "            from %s box\n",
      "            where ST_ContainsProperly(box,v.geom)\n",
      "            \"\"\"%(str(box_num),box_txt))\n",
      "\n",
      "        T.update({ 'b_l_pt':T_tmp['t_l_pt'].replace('t_l_pt','b_l_pt'),\n",
      "                   'b_r_pt':T_tmp['t_r_pt'].replace('t_r_pt','b_r_pt'),\n",
      "                   't_l_pt':T_tmp['t_l_pt'],'t_r_pt':T_tmp['t_r_pt'],})\n",
      "\n",
      "        box_num += 1\n",
      "\n",
      "        if last_move=='complete':\n",
      "            break\n",
      "\n",
      "    if show_info==True: gd.GeoSeries(base_geom+saved_boxes).plot()\n",
      "    \n",
      "    return saved_boxes\n",
      "\n",
      "saved_geom = update_map_groups(min_v=1800.0,max_v=2000.0,\n",
      "                               base_geom=base_geom,show_info=True)\n",
      "gd.GeoSeries(base_geom+saved_geom).plot()"
     ],
     "language": "python",
     "metadata": {
      "code_folding": [
       0
      ],
      "run_control": {
       "read_only": false
      }
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# DB Check:\n",
      "c1=\"\"\"\n",
      "select count(*) \n",
      "from \n",
      "    yelp_geom_error yg,\n",
      "    (select array_agg(y.id) A from yelp y) as f1 \n",
      "where (not array[yg.id] && A)\n",
      "and non_mn_zipcode is false\n",
      "\"\"\"\n",
      "# 3548 --  6 every 30 sec, 12 every min\n",
      "c2=\"\"\"\n",
      "update seamless set camis = null\n",
      "\"\"\"\n",
      "c3=\"\"\"\n",
      "select count(*) from yelp;\n",
      "\"\"\"\n",
      "%sql postgres@routing select count(*) from yelp_geom_error where postal_code is null\n",
      "# (4113/12.0)/60"
     ],
     "language": "python",
     "metadata": {
      "code_folding": [
       0
      ],
      "run_control": {
       "read_only": false
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1 rows affected.\n"
       ]
      },
      {
       "html": [
        "<table>\n",
        "    <tr>\n",
        "        <th>count</th>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td>9874</td>\n",
        "    </tr>\n",
        "</table>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 76,
       "text": [
        "[(9874L,)]"
       ]
      }
     ],
     "prompt_number": 76
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "# Create KML\n",
      "\"\"\"\n",
      "See here: https://developers.google.com/kml/documentation/kmlelementsinmaps\n",
      "\"\"\""
     ],
     "language": "python",
     "metadata": {
      "code_folding": [
       0
      ],
      "run_control": {
       "read_only": false
      }
     },
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}